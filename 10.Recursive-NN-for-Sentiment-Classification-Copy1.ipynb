{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.Tree Recursive Neural Networks and Constituency Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture14-TreeRNNs.pdf\n",
    "* https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter, OrderedDict\n",
    "import nltk\n",
    "from copy import deepcopy\n",
    "import os\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if USE_CUDA else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if USE_CUDA else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if USE_CUDA else torch.ByteTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getBatch(batch_size,train_data):\n",
    "    random.shuffle(train_data)\n",
    "    sindex=0\n",
    "    eindex=batch_size\n",
    "    while eindex < len(train_data):\n",
    "        batch = train_data[sindex:eindex]\n",
    "        temp = eindex\n",
    "        eindex = eindex+batch_size\n",
    "        sindex = temp\n",
    "        yield batch\n",
    "    \n",
    "    if eindex >= len(train_data):\n",
    "        batch = train_data[sindex:]\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data load and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stanford Sentiment Treebank(https://nlp.stanford.edu/sentiment/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0 (1 (1 (1 (0 (0 (2 (2 Our) (2 culture)) (0 (2 is) (0 (0 (2 headed) (1 (2 (2 down) (2 (2 the) (1 toilet))) (1 (2 with) (2 (2 (2 the) (2 ferocity)) (2 (2 of) (2 (2 a) (2 (2 frozen) (2 burrito)))))))) (2 (2 after) (2 (2 an) (2 (1 all-night) (3 (2 tequila) (2 bender)))))))) (2 --)) (2 and)) (2 (2 I) (2 (2 (2 know) (2 this)) (2 (2 because) (2 (2 I) (2 (2 've) (2 (2 seen) (2 (2 `) (2 (2 (1 jackass) (2 :)) (2 (2 the) (2 movie))))))))))) (2 .)) (2 '))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample = random.choice(open('../dataset/trees/train.txt','r',encoding='utf-8').readlines())\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree Class "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "borrowed code from https://github.com/bogatyy/cs224d/tree/master/assignment3 (thanks!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Node:  # a node in the tree\n",
    "    def __init__(self, label, word=None):\n",
    "        self.label = label\n",
    "        self.word = word\n",
    "        self.parent = None  # reference to parent\n",
    "        self.left = None  # reference to left child\n",
    "        self.right = None  # reference to right child\n",
    "        # true if I am a leaf (could have probably derived this from if I have\n",
    "        # a word)\n",
    "        self.isLeaf = False\n",
    "        # true if we have finished performing fowardprop on this node (note,\n",
    "        # there are many ways to implement the recursion.. some might not\n",
    "        # require this flag)\n",
    "\n",
    "    def __str__(self):\n",
    "        if self.isLeaf:\n",
    "            return '[{0}:{1}]'.format(self.word, self.label)\n",
    "        return '({0} <- [{1}:{2}] -> {3})'.format(self.left, self.word, self.label, self.right)\n",
    "\n",
    "\n",
    "class Tree:\n",
    "\n",
    "    def __init__(self, treeString, openChar='(', closeChar=')'):\n",
    "        tokens = []\n",
    "        self.open = '('\n",
    "        self.close = ')'\n",
    "        for toks in treeString.strip().split():\n",
    "            tokens += list(toks)\n",
    "        self.root = self.parse(tokens)\n",
    "        # get list of labels as obtained through a post-order traversal\n",
    "        self.labels = get_labels(self.root)\n",
    "        self.num_words = len(self.labels)\n",
    "\n",
    "    def parse(self, tokens, parent=None):\n",
    "        assert tokens[0] == self.open, \"Malformed tree\"\n",
    "        assert tokens[-1] == self.close, \"Malformed tree\"\n",
    "\n",
    "        split = 2  # position after open and label\n",
    "        countOpen = countClose = 0\n",
    "\n",
    "        if tokens[split] == self.open:\n",
    "            countOpen += 1\n",
    "            split += 1\n",
    "        # Find where left child and right child split\n",
    "        while countOpen != countClose:\n",
    "            if tokens[split] == self.open:\n",
    "                countOpen += 1\n",
    "            if tokens[split] == self.close:\n",
    "                countClose += 1\n",
    "            split += 1\n",
    "\n",
    "        # New node\n",
    "        node = Node(int(tokens[1]))  # zero index labels\n",
    "\n",
    "        node.parent = parent\n",
    "\n",
    "        # leaf Node\n",
    "        if countOpen == 0:\n",
    "            node.word = ''.join(tokens[2:-1]).lower()  # lower case?\n",
    "            node.isLeaf = True\n",
    "            return node\n",
    "\n",
    "        node.left = self.parse(tokens[2:split], parent=node)\n",
    "        node.right = self.parse(tokens[split:-1], parent=node)\n",
    "\n",
    "        return node\n",
    "\n",
    "    def get_words(self):\n",
    "        leaves = getLeaves(self.root)\n",
    "        words = [node.word for node in leaves]\n",
    "        return words\n",
    "\n",
    "def get_labels(node):\n",
    "    if node is None:\n",
    "        return []\n",
    "    return get_labels(node.left) + get_labels(node.right) + [node.label]\n",
    "\n",
    "def getLeaves(node):\n",
    "    if node is None:\n",
    "        return []\n",
    "    if node.isLeaf:\n",
    "        return [node]\n",
    "    else:\n",
    "        return getLeaves(node.left) + getLeaves(node.right)\n",
    "\n",
    "def loadTrees(dataSet='train'):\n",
    "    \"\"\"\n",
    "    Loads training trees. Maps leaf node words to word ids.\n",
    "    \"\"\"\n",
    "    file = '../dataset/trees/%s.txt' % dataSet\n",
    "    print(\"Loading %s trees..\" % dataSet)\n",
    "    with open(file, 'r',encoding='utf-8') as fid:\n",
    "        trees = [Tree(l) for l in fid.readlines()]\n",
    "\n",
    "    return trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train trees..\n"
     ]
    }
   ],
   "source": [
    "train_data = loadTrees('train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Vocab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = list(set(flatten([t.get_words() for t in train_data])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2index={'<UNK>':0}\n",
    "for vo in vocab:\n",
    "    if vo not in word2index.keys():\n",
    "        word2index[vo]=len(word2index)\n",
    "        \n",
    "index2word = {v:k for k,v in word2index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RNTN(nn.Module):\n",
    "    \n",
    "    def __init__(self,word2index,hidden_size,output_size):\n",
    "        super(RNTN,self).__init__()\n",
    "        \n",
    "        self.word2index = word2index\n",
    "        self.embed = nn.Embedding(len(word2index),hidden_size)\n",
    "#         self.V = nn.ModuleList([nn.Linear(hidden_size*2,hidden_size*2) for _ in range(hidden_size)])\n",
    "#         self.W = nn.Linear(hidden_size*2,hidden_size)\n",
    "        self.V = nn.ParameterList([nn.Parameter(torch.randn(hidden_size*2,hidden_size*2)) for _ in range(hidden_size)]) # Tensor\n",
    "        self.W = nn.Parameter(torch.randn(hidden_size*2,hidden_size))\n",
    "#         self.b = nn.Parameter(torch.randn(1,hidden_size))\n",
    "        self.W_out = nn.Linear(hidden_size,output_size)\n",
    "        \n",
    "    def init_weight(self):\n",
    "        nn.init.xavier_uniform(self.embed.state_dict()['weight'])\n",
    "        nn.init.xavier_normal(self.W_out.state_dict()['weight'])\n",
    "#         self.W_out.bias.data.fill_(0)\n",
    "        for param in self.V.parameters():\n",
    "            nn.init.xavier_normal(param)\n",
    "        nn.init.xavier_normal(self.W)\n",
    "        \n",
    "    def tree_propagation(self,node):\n",
    "        \n",
    "        recursive_tensor = OrderedDict()\n",
    "        current=None\n",
    "        if node.isLeaf:\n",
    "            tensor = Variable(LongTensor([self.word2index[node.word]])) if node.word in self.word2index.keys() \\\n",
    "                          else Variable(LongTensor([self.word2index['<UNK>']]))\n",
    "            current = self.embed(tensor) # 1xD\n",
    "        else:\n",
    "            recursive_tensor.update(self.tree_propagation(node.left))\n",
    "            recursive_tensor.update(self.tree_propagation(node.right))\n",
    "            \n",
    "            concated = torch.cat([recursive_tensor[node.left],recursive_tensor[node.right]],1) # 1x2D\n",
    "            xVx=[] \n",
    "            for i,v in enumerate(self.V):\n",
    "#                 xVx.append(torch.matmul(v(concated),concated.transpose(0,1)))\n",
    "                xVx.append(torch.matmul(torch.matmul(concated,v),concated.transpose(0,1)))\n",
    "            \n",
    "            xVx = torch.cat(xVx,1) # 1xD\n",
    "#             Wx = self.W(concated)\n",
    "            Wx = torch.matmul(concated,self.W) # 1xD\n",
    "\n",
    "            current = F.tanh(xVx+Wx) # 1xD\n",
    "        recursive_tensor[node]=current\n",
    "        return recursive_tensor\n",
    "        \n",
    "    def forward(self,Trees,root_only=False):\n",
    "        \n",
    "        propagated=[]\n",
    "        if not isinstance(Trees,list):\n",
    "            Trees = [Trees]\n",
    "            \n",
    "        for Tree in Trees:\n",
    "            recursive_tensor = self.tree_propagation(Tree.root)\n",
    "            if root_only:\n",
    "                recursive_tensor = recursive_tensor[Tree.root]\n",
    "                propagated.append(recursive_tensor)\n",
    "            else:\n",
    "                recursive_tensor = [tensor for node,tensor in recursive_tensor.items()]\n",
    "                propagated.extend(recursive_tensor)\n",
    "        \n",
    "        propagated = torch.cat(propagated)\n",
    "              \n",
    "        return F.log_softmax(self.W_out(propagated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes for a while... It makes computational graph dynamically. So Its computation is difficult to train with batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 30\n",
    "ROOT_ONLY = False\n",
    "BATCH_SIZE = 20\n",
    "EPOCH = 10\n",
    "LR = 0.01\n",
    "LAMBDA = 1e-5\n",
    "RESCHEDULED=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = RNTN(word2index,HIDDEN_SIZE,5)\n",
    "model.init_weight()\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=LR) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/10] mean_loss : 1.63\n",
      "[0/10] mean_loss : 1.25\n",
      "[0/10] mean_loss : 0.98\n",
      "[0/10] mean_loss : 0.95\n",
      "[0/10] mean_loss : 0.94\n",
      "[1/10] mean_loss : 0.97\n",
      "[1/10] mean_loss : 0.94\n",
      "[1/10] mean_loss : 0.92\n",
      "[1/10] mean_loss : 0.92\n",
      "[1/10] mean_loss : 0.94\n",
      "[2/10] mean_loss : 0.86\n",
      "[2/10] mean_loss : 0.93\n",
      "[2/10] mean_loss : 0.93\n",
      "[2/10] mean_loss : 0.92\n",
      "[2/10] mean_loss : 0.94\n",
      "[3/10] mean_loss : 0.95\n",
      "[3/10] mean_loss : 0.92\n",
      "[3/10] mean_loss : 0.93\n",
      "[3/10] mean_loss : 0.94\n",
      "[3/10] mean_loss : 0.92\n",
      "[4/10] mean_loss : 0.97\n",
      "[4/10] mean_loss : 0.92\n",
      "[4/10] mean_loss : 0.93\n",
      "[4/10] mean_loss : 0.92\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-ea0ef688af09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \"\"\"\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    losses=[]\n",
    "    # Learning rate annealing\n",
    "    if RESCHEDULED==False and epoch==EPOCH//2:\n",
    "        LR = LR*0.1 # 0.001\n",
    "        optimizer = optim.Adam(model.parameters(),lr=LR,weight_decay=LAMBDA) # L2 norm\n",
    "        RESCHEDULED=True\n",
    "    \n",
    "    for i, batch in enumerate(getBatch(BATCH_SIZE,train_data)):\n",
    "        \n",
    "        if ROOT_ONLY:\n",
    "            labels = [tree.labels[-1] for tree in batch]\n",
    "            labels = Variable(LongTensor(labels))\n",
    "        else:\n",
    "            labels = [tree.labels for tree in batch]\n",
    "            labels = Variable(LongTensor(flatten(labels)))\n",
    "        \n",
    "        model.zero_grad()\n",
    "        preds = model(batch,ROOT_ONLY)\n",
    "        \n",
    "        loss = loss_function(preds,labels)\n",
    "        losses.append(loss.data.tolist()[0])\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 100==0:\n",
    "            print('[%d/%d] mean_loss : %.2f' % (epoch,EPOCH,np.mean(losses)))\n",
    "            losses=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test trees..\n"
     ]
    }
   ],
   "source": [
    "test_data = loadTrees('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy=0\n",
    "num_node=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-grained all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In paper, they acheived 80.2 accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77.93911771537626\n"
     ]
    }
   ],
   "source": [
    "for test in test_data:\n",
    "    model.zero_grad()\n",
    "    preds = model(test,ROOT_ONLY)\n",
    "    labels = test.labels[-1:] if ROOT_ONLY else test.labels\n",
    "    for pred,label in zip(preds.max(1)[1].data.tolist(),labels):\n",
    "        num_node+=1\n",
    "        if pred==label:\n",
    "            accuracy+=1\n",
    "\n",
    "print(accuracy/num_node*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://github.com/nearai/pytorch-tools # Dynamic batch using TensorFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
