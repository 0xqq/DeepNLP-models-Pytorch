{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Dependency Parsing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture6.pdf\n",
    "* http://cs.stanford.edu/people/danqi/papers/emnlp2014.pdf\n",
    "* https://github.com/rguthrie3/DeepDependencyParsingProblemSet/tree/master/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter, OrderedDict\n",
    "import nltk\n",
    "from nltk.tree import Tree\n",
    "import os\n",
    "from IPython.display import Image, display\n",
    "from nltk.draw import TreeWidget\n",
    "from nltk.draw.util import CanvasFrame\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if USE_CUDA else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if USE_CUDA else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if USE_CUDA else torch.ByteTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getBatch(batch_size,train_data):\n",
    "    random.shuffle(train_data)\n",
    "    sindex=0\n",
    "    eindex=batch_size\n",
    "    while eindex < len(train_data):\n",
    "        batch = train_data[sindex:eindex]\n",
    "        temp = eindex\n",
    "        eindex = eindex+batch_size\n",
    "        sindex = temp\n",
    "        yield batch\n",
    "    \n",
    "    if eindex >= len(train_data):\n",
    "        batch = train_data[sindex:]\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_index):\n",
    "    idxs = list(map(lambda w: to_index[w] if w in to_index.keys() else to_index[\"<unk>\"], seq))\n",
    "    return Variable(LongTensor(idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Borrowed from https://stackoverflow.com/questions/31779707/how-do-you-make-nltk-draw-trees-that-are-inline-in-ipython-jupyter\n",
    "\n",
    "def draw_nltk_tree(tree):\n",
    "    cf = CanvasFrame()\n",
    "    tc = TreeWidget(cf.canvas(), tree)\n",
    "    tc['node_font'] = 'arial 15 bold'\n",
    "    tc['leaf_font'] = 'arial 15'\n",
    "    tc['node_color'] = '#005990'\n",
    "    tc['leaf_color'] = '#3F8F57'\n",
    "    tc['line_color'] = '#175252'\n",
    "    cf.add_widget(tc, 50, 50)\n",
    "    cf.print_to_file('tmp_tree_output.ps')\n",
    "    cf.destroy()\n",
    "    os.system('convert tmp_tree_output.ps tmp_tree_output.png')\n",
    "    display(Image(filename='tmp_tree_output.png'))\n",
    "    os.system('rm tmp_tree_output.ps tmp_tree_output.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition State Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TransitionState(object):\n",
    "    \n",
    "    def __init__(self,tagged_sent):\n",
    "        self.root = ('ROOT','<root>',-1)\n",
    "        self.stack=[self.root]\n",
    "        self.buffer=[(s[0],s[1],i) for i,s in enumerate(tagged_sent)]\n",
    "        self.address = [s[0] for s in tagged_sent] + [self.root[0]]\n",
    "        self.arcs=[]\n",
    "        self.terminal=False\n",
    "        \n",
    "    def __str__(self):\n",
    "        return 'stack : %s \\nbuffer : %s' % (str([s[0] for s in self.stack]), str([b[0] for b in self.buffer]))\n",
    "    \n",
    "    def shift(self):\n",
    "        \n",
    "        if len(self.buffer)>=1:\n",
    "            self.stack.append(self.buffer.pop(0))\n",
    "        else:\n",
    "            print(\"Empty buffer\")\n",
    "            \n",
    "    def left_arc(self,relation=None):\n",
    "        \n",
    "        if len(self.stack)>=2:\n",
    "            arc={}\n",
    "            s2 = self.stack[-2]\n",
    "            s1 = self.stack[-1]\n",
    "            arc['graph_id'] = len(self.arcs)\n",
    "            arc['form'] = s1[0]\n",
    "            arc['addr'] = s1[2]\n",
    "            arc['head']=s2[2]\n",
    "            arc['pos'] = s1[1]\n",
    "            if relation:\n",
    "                arc['relation']=relation\n",
    "            self.arcs.append(arc)\n",
    "            self.stack.pop(-2)\n",
    "            \n",
    "        elif self.stack==[self.root]:\n",
    "            print(\"Element Lacking\")\n",
    "    \n",
    "    def right_arc(self,relation=None):\n",
    "        \n",
    "        if len(self.stack)>=2:\n",
    "            arc={}\n",
    "            s2 = self.stack[-2]\n",
    "            s1 = self.stack[-1]\n",
    "            arc['graph_id'] = len(self.arcs)\n",
    "            arc['form'] = s2[0]\n",
    "            arc['addr'] = s2[2]\n",
    "            arc['head']=s1[2]\n",
    "            arc['pos'] = s2[1]\n",
    "            if relation:\n",
    "                arc['relation']=relation\n",
    "            self.arcs.append(arc)\n",
    "            self.stack.pop(-1)\n",
    "            \n",
    "        elif self.stack==[self.root]:\n",
    "            print(\"Element Lacking\")\n",
    "    \n",
    "    def get_left_most(self,index):\n",
    "        left=['<NULL>','<NULL>',None]\n",
    "        \n",
    "        if index==None: return left\n",
    "        for arc in self.arcs:\n",
    "            if arc['head']==index:\n",
    "                left=[arc['form'],arc['pos'],arc['addr']]\n",
    "                break\n",
    "        return left\n",
    "    \n",
    "    def get_right_most(self,index):\n",
    "        right=['<NULL>','<NULL>',None]\n",
    "        \n",
    "        if index==None: return right\n",
    "        for arc in reversed(self.arcs):\n",
    "            if arc['head']==index:\n",
    "                right=[arc['form'],arc['pos'],arc['addr']]\n",
    "                break\n",
    "        return right\n",
    "    \n",
    "    def is_done(self):\n",
    "        return len(self.buffer)==0 and self.stack==[self.root]\n",
    "    \n",
    "    def to_tree_string(self):\n",
    "        if self.is_done()==False: return None\n",
    "        ingredient=[]\n",
    "        for arc in self.arcs:\n",
    "            ingredient.append([arc['form'],self.address[arc['head']]])\n",
    "        ingredient = ingredient[-1:] + ingredient[:-1]\n",
    "        return self._make_tree(ingredient,0)\n",
    "    \n",
    "    def _make_tree(self,ingredient,i,new=True):\n",
    "    \n",
    "        if new:\n",
    "            treestr=\"(\"\n",
    "            treestr+=ingredient[i][0]\n",
    "            treestr+=\" \"\n",
    "        else:\n",
    "            treestr=\"\"\n",
    "        ingredient[i][0]=\"CHECK\"\n",
    "\n",
    "        parents,_ = list(zip(*ingredient))\n",
    "\n",
    "        if ingredient[i][1] not in parents:\n",
    "            treestr+=ingredient[i][1]\n",
    "            return treestr\n",
    "\n",
    "        else:\n",
    "            treestr+=\"(\"\n",
    "            treestr+=ingredient[i][1]\n",
    "            treestr+=\" \"\n",
    "            for node_i,node in enumerate(parents):\n",
    "                if node==ingredient[i][1]:\n",
    "                    treestr+=self._make_tree(ingredient,node_i,False)\n",
    "                    treestr+=\" \"\n",
    "\n",
    "            treestr = treestr.strip()\n",
    "            treestr+=\")\"\n",
    "        if new:\n",
    "            treestr+=\")\"\n",
    "        return treestr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of transition-based dependancy parsing in the paper. Model's goal is to predict correct transition of parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack : ['ROOT'] \n",
      "buffer : ['He', 'has', 'good', 'control', '.']\n",
      "stack : ['ROOT', 'He', 'has'] \n",
      "buffer : ['good', 'control', '.']\n",
      "stack : ['ROOT', 'has'] \n",
      "buffer : ['good', 'control', '.']\n",
      "[{'head': 0, 'pos': 'VBZ', 'addr': 1, 'form': 'has', 'graph_id': 0}]\n",
      "stack : ['ROOT', 'has', 'good', 'control'] \n",
      "buffer : ['.']\n",
      "stack : ['ROOT', 'has', 'control'] \n",
      "buffer : ['.']\n",
      "stack : ['ROOT', 'has'] \n",
      "buffer : ['.']\n",
      "stack : ['ROOT', 'has'] \n",
      "buffer : []\n",
      "stack : ['ROOT'] \n",
      "buffer : []\n",
      "[{'head': 0, 'pos': 'VBZ', 'addr': 1, 'form': 'has', 'graph_id': 0}, {'head': 2, 'pos': 'NN', 'addr': 3, 'form': 'control', 'graph_id': 1}, {'head': 3, 'pos': 'VBZ', 'addr': 1, 'form': 'has', 'graph_id': 2}, {'head': 4, 'pos': 'VBZ', 'addr': 1, 'form': 'has', 'graph_id': 3}, {'head': 1, 'pos': '<root>', 'addr': -1, 'form': 'ROOT', 'graph_id': 4}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = TransitionState(nltk.pos_tag(\"He has good control .\".split()))\n",
    "print(state)\n",
    "state.shift()\n",
    "state.shift()\n",
    "print(state)\n",
    "state.left_arc()\n",
    "print(state)\n",
    "print(state.arcs)\n",
    "state.shift()\n",
    "state.shift()\n",
    "print(state)\n",
    "state.left_arc()\n",
    "print(state)\n",
    "state.right_arc()\n",
    "print(state)\n",
    "state.shift()\n",
    "state.right_arc()\n",
    "print(state)\n",
    "state.right_arc()\n",
    "print(state)\n",
    "print(state.arcs)\n",
    "state.is_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(ROOT (has He (control good) .))'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.to_tree_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "draw_nltk_tree(Tree.fromstring(state.to_tree_string()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data load & Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get features from stack and buffer. <br>\n",
    "1.  The top 3 words on the stack and buffer ($s_1,s_2,s_3,b_1,b_2,b_3$)\n",
    "2. The first and second leftmost / rightmost children of the top two words on the stack: $lc_1(s_i), rc_1(s_i), lTc_2(s_i), rc_2(s_i), i = 1, 2$\n",
    "3. <i>The leftmost of leftmost / rightmost of rightmost children of the top two words on the stack: $lc_1(lc_1(s_i)), rc_1(rc_1(s_i)), i = 1, 2$. # I don't use these features</i>\n",
    "4. POS tags for $S^t$\n",
    "5. <i>corresponding arc labels of words excluding those 6 words on the stack/buffer for $S^l$ # I don't use these features</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feat(transition_state,word2index,tag2index,label2index=None):\n",
    "    word_feats=[]\n",
    "    tag_feats = []\n",
    "    \n",
    "    word_feats.append(transition_state.stack[-1][0]) if len(transition_state.stack)>=1 and \\\n",
    "    transition_state.stack[-1][0] in word2index.keys() else word_feats.append('<NULL>') # s1\n",
    "    word_feats.append(transition_state.stack[-2][0]) if len(transition_state.stack)>=2 and \\\n",
    "    transition_state.stack[-2][0] in word2index.keys() else word_feats.append('<NULL>') # s2\n",
    "    word_feats.append(transition_state.stack[-3][0]) if len(transition_state.stack)>=3 and \\\n",
    "    transition_state.stack[-3][0] in word2index.keys() else word_feats.append('<NULL>') # s3\n",
    "    \n",
    "    tag_feats.append(transition_state.stack[-1][1]) if len(transition_state.stack)>=1 and \\\n",
    "    transition_state.stack[-1][1] in tag2index.keys() else tag_feats.append('<NULL>') # st1\n",
    "    tag_feats.append(transition_state.stack[-2][1]) if len(transition_state.stack)>=2 and \\\n",
    "    transition_state.stack[-2][1] in tag2index.keys() else tag_feats.append('<NULL>') # st2\n",
    "    tag_feats.append(transition_state.stack[-3][1]) if len(transition_state.stack)>=3 and \\\n",
    "    transition_state.stack[-3][1] in tag2index.keys() else tag_feats.append('<NULL>') # st3\n",
    "    \n",
    "    \n",
    "    word_feats.append(transition_state.buffer[0][0]) if len(transition_state.buffer)>=1 and \\\n",
    "    transition_state.buffer[0][0] in word2index.keys() else word_feats.append('<NULL>') # b1\n",
    "    word_feats.append(transition_state.buffer[1][0]) if len(transition_state.buffer)>=2 and \\\n",
    "    transition_state.buffer[1][0] in word2index.keys() else word_feats.append('<NULL>') # b2\n",
    "    word_feats.append(transition_state.buffer[2][0]) if len(transition_state.buffer)>=3 and \\\n",
    "    transition_state.buffer[2][0] in word2index.keys() else word_feats.append('<NULL>') # b3\n",
    "    \n",
    "    tag_feats.append(transition_state.buffer[0][1]) if len(transition_state.buffer)>=1 and \\\n",
    "    transition_state.buffer[0][1] in tag2index.keys() else tag_feats.append('<NULL>') # bt1\n",
    "    tag_feats.append(transition_state.buffer[1][1]) if len(transition_state.buffer)>=2 and \\\n",
    "    transition_state.buffer[1][1] in tag2index.keys() else tag_feats.append('<NULL>') # bt2\n",
    "    tag_feats.append(transition_state.buffer[2][1]) if len(transition_state.buffer)>=3 and \\\n",
    "    transition_state.buffer[2][1] in tag2index.keys() else tag_feats.append('<NULL>') # bt3\n",
    "    \n",
    "    \n",
    "    lc_s1 = transition_state.get_left_most(transition_state.stack[-1][2]) if len(transition_state.stack)>=1 \\\n",
    "    else transition_state.get_left_most(None)\n",
    "    rc_s1 = transition_state.get_right_most(transition_state.stack[-1][2]) if len(transition_state.stack)>=1 \\\n",
    "    else transition_state.get_right_most(None)\n",
    "    \n",
    "    lc_s2 = transition_state.get_left_most(transition_state.stack[-2][2]) if len(transition_state.stack)>=2 \\\n",
    "    else transition_state.get_left_most(None)\n",
    "    rc_s2 = transition_state.get_right_most(transition_state.stack[-2][2]) if len(transition_state.stack)>=2 \\\n",
    "    else transition_state.get_right_most(None)\n",
    "    \n",
    "    words, tags, _ = zip(*[lc_s1,rc_s1,lc_s2,rc_s2])\n",
    "    \n",
    "    word_feats.extend(words)\n",
    "    \n",
    "    tag_feats.extend(tags)\n",
    "    \n",
    "    \n",
    "    return prepare_sequence(word_feats,word2index).view(1,-1), prepare_sequence(tag_feats,tag2index).view(1,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get data from <a href=\"https://github.com/rguthrie3/DeepDependencyParsingProblemSet\">this repo</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = open('../dataset/dparser/train.txt','r').readlines()\n",
    "vocab = open('../dataset/dparser/vocab.txt','r').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "splited_data = [[nltk.pos_tag(d.split('|||')[0].split()), d.split('|||')[1][:-1].split()] for d in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x,train_y = list(zip(*splited_data))\n",
    "train_x_f = flatten(train_x)\n",
    "sents,pos_tags = list(zip(*train_x_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag2index = {v:i for i,v in enumerate(set(pos_tags))}\n",
    "tag2index['<root>']=len(tag2index)\n",
    "tag2index['<NULL>']=len(tag2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = [v.split('\\t')[0] for v in vocab]\n",
    "word2index = {v:i for i,v in enumerate(vocab)}\n",
    "word2index['ROOT'] = len(word2index)\n",
    "word2index['<NULL>'] = len(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "actions = ['SHIFT','REDUCE_L','REDUCE_R']\n",
    "action2index = {v:i for i,v in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for tx,ty in splited_data:\n",
    "    state = TransitionState(tx)\n",
    "    transition = ty+['REDUCE_R'] # root\n",
    "    while len(transition)!=0:\n",
    "        feat = get_feat(state,word2index,tag2index)\n",
    "        action = transition.pop(0)\n",
    "        actionTensor = Variable(LongTensor([action2index[action]])).view(1,-1)\n",
    "        train_data.append([feat,actionTensor])\n",
    "        if action=='SHIFT':\n",
    "            state.shift()\n",
    "        elif action=='REDUCE_R':\n",
    "            state.right_arc()\n",
    "        elif action=='REDUCE_L':\n",
    "            state.left_arc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Variable containing:\n",
       "   9151  9152  9152  2106     2   353  9152  9152  9152  9152\n",
       "  [torch.LongTensor of size 1x10], Variable containing:\n",
       "     43    44    44    21    28    29    44    44    44    44\n",
       "  [torch.LongTensor of size 1x10]), Variable containing:\n",
       "  0\n",
       " [torch.LongTensor of size 1x1]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralDependencyParser(nn.Module):\n",
    "    \n",
    "    def __init__(self,w_size,w_embed_dim,t_size,t_embed_dim,hidden_size,target_size):\n",
    "        \n",
    "        super(NeuralDependencyParser, self).__init__()\n",
    "        \n",
    "        self.w_embed =  nn.Embedding(w_size,w_embed_dim)\n",
    "        self.t_embed = nn.Embedding(t_size,t_embed_dim)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.target_size = target_size\n",
    "        self.linear = nn.Linear((w_embed_dim+t_embed_dim)*10,self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size,self.target_size)\n",
    "        \n",
    "        self.w_embed.weight.data.uniform_(-0.01, 0.01) # init\n",
    "        self.t_embed.weight.data.uniform_(-0.01, 0.01) # init\n",
    "        \n",
    "    def forward(self,words,tags):\n",
    "        \n",
    "        wem = self.w_embed(words).view(words.size(0),-1)\n",
    "        tem = self.t_embed(tags).view(tags.size(0),-1)\n",
    "        inputs = torch.cat([wem,tem],1)\n",
    "        h1 = torch.pow(self.linear(inputs),3) # cube activation function\n",
    "        preds = -self.out(h1)\n",
    "        return F.log_softmax(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "STEP = 5\n",
    "BATCH_SIZE=256\n",
    "W_EMBED_SIZE = 50\n",
    "T_EMBED_SIZE = 10\n",
    "HIDDEN_SIZE=512\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = NeuralDependencyParser(len(word2index),W_EMBED_SIZE,len(tag2index),T_EMBED_SIZE,HIDDEN_SIZE,len(action2index))\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "losses=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_loss : 1.10\n",
      "mean_loss : 0.81\n",
      "mean_loss : 0.38\n",
      "mean_loss : 0.32\n",
      "mean_loss : 0.30\n",
      "mean_loss : 0.27\n",
      "mean_loss : 0.26\n",
      "mean_loss : 0.25\n",
      "mean_loss : 0.24\n",
      "mean_loss : 0.23\n",
      "mean_loss : 0.21\n",
      "mean_loss : 0.22\n",
      "mean_loss : 0.21\n",
      "mean_loss : 0.20\n",
      "mean_loss : 0.20\n",
      "mean_loss : 0.19\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(getBatch(BATCH_SIZE,train_data)):\n",
    "    \n",
    "    model.zero_grad()\n",
    "    inputs, targets = list(zip(*batch))\n",
    "    words, tags = list(zip(*inputs))\n",
    "    words = torch.cat(words)\n",
    "    tags = torch.cat(tags)\n",
    "    targets = torch.cat(targets)\n",
    "    preds = model(words,tags)\n",
    "    loss = loss_function(preds,targets.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(loss.data.tolist()[0])\n",
    "    \n",
    "    if i % 100==0:\n",
    "        print(\"mean_loss : %0.2f\" %(np.mean(losses)))\n",
    "        losses=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test (UAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev = open('../dataset/dparser/dev.txt','r').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "splited_data = [[nltk.pos_tag(d.split('|||')[0].split()), d.split('|||')[1][:-1].split()] for d in dev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_data=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for tx,ty in splited_data:\n",
    "    state = TransitionState(tx)\n",
    "    transition = ty+['REDUCE_R'] # root\n",
    "    while len(transition)!=0:\n",
    "        feat = get_feat(state,word2index,tag2index)\n",
    "        action = transition.pop(0)\n",
    "        dev_data.append([feat,action2index[action]])\n",
    "        if action=='SHIFT':\n",
    "            state.shift()\n",
    "        elif action=='REDUCE_R':\n",
    "            state.right_arc()\n",
    "        elif action=='REDUCE_L':\n",
    "            state.left_arc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.50124069478908\n"
     ]
    }
   ],
   "source": [
    "for dev in dev_data:\n",
    "    input, target = dev[0], dev[1]\n",
    "    word, tag = input[0], input[1]\n",
    "    pred = model(word,tag).max(1)[1]\n",
    "    pred = pred.data.tolist()[0]\n",
    "    if pred==target:\n",
    "        accuracy+=1\n",
    "\n",
    "print(accuracy/len(dev_data)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = TransitionState(nltk.pos_tag(\"I shot an elephant in my pajamas\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index2action = {i:v for v,i in action2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "while test.is_done()==False:\n",
    "    feat = get_feat(test,word2index,tag2index)\n",
    "    word,tag = feat[0],feat[1]\n",
    "    action = model(word,tag).max(1)[1].data.tolist()[0]\n",
    "    \n",
    "    action = index2action[action]\n",
    "    \n",
    "    if action=='SHIFT':\n",
    "        test.shift()\n",
    "    elif action=='REDUCE_R':\n",
    "        test.right_arc()\n",
    "    elif action=='REDUCE_L':\n",
    "        test.left_arc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack : ['ROOT'] \n",
      "buffer : []\n"
     ]
    }
   ],
   "source": [
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.to_tree_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "draw_nltk_tree(Tree.fromstring(test.to_tree_string()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
