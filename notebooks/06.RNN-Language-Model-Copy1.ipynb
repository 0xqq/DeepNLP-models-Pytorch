{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Recurrent Neural Networks and Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture8.pdf\n",
    "* https://arxiv.org/pdf/1504.00941.pdf\n",
    "* https://arxiv.org/pdf/1609.07843.pdf\n",
    "* https://github.com/pytorch/examples/tree/master/word_language_model\n",
    "* https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/language_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter, OrderedDict\n",
    "import nltk\n",
    "from copy import deepcopy\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if USE_CUDA else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if USE_CUDA else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if USE_CUDA else torch.ByteTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_index):\n",
    "    idxs = list(map(lambda w: to_index[w] if w in to_index.keys() else to_index[\"<unk>\"], seq))\n",
    "    return LongTensor(idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data load and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_ptb_dataset(filename,word2index=None):\n",
    "    corpus = open(filename,'r',encoding='utf-8').readlines()\n",
    "    corpus = flatten([co.strip().split() + ['</s>'] for co in corpus])\n",
    "    \n",
    "    if word2index==None:\n",
    "        vocab = list(set(corpus))\n",
    "        word2index={'<unk>':0}\n",
    "        for vo in vocab:\n",
    "            if vo not in word2index.keys():\n",
    "                word2index[vo]=len(word2index)\n",
    "    \n",
    "    return prepare_sequence(corpus,word2index), word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# borrowed code from https://github.com/pytorch/examples/tree/master/word_language_model\n",
    "\n",
    "def batchify(data, bsz):\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).contiguous()\n",
    "    if USE_CUDA:\n",
    "        data = data.cuda()\n",
    "    return data\n",
    "\n",
    "def get_batch(source, i, evaluation=False):\n",
    "    seq_len = min(SEQ_LENGTH, len(source) - 1 - i)\n",
    "    data = Variable(source[i:i+seq_len], volatile=evaluation)\n",
    "    target = Variable(source[i+1:i+1+seq_len].view(-1))\n",
    "    return data, target\n",
    "\n",
    "\n",
    "def evaluate(data_source):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    for i in range(0, data_source.size(0) - 1, args.bptt):\n",
    "        data, targets = get_batch(data_source, i, evaluation=True)\n",
    "        output, hidden = model(data, hidden)\n",
    "        output_flat = output.view(-1, ntokens)\n",
    "        total_loss += len(data) * criterion(output_flat, targets).data\n",
    "        hidden = repackage_hidden(hidden)\n",
    "    return total_loss[0] / len(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data, word2index= prepare_ptb_dataset('../dataset/ptb/ptb.train.txt',)\n",
    "dev_data , _ = prepare_ptb_dataset('../dataset/ptb/ptb.valid.txt',word2index)\n",
    "test_data, _ = prepare_ptb_dataset('../dataset/ptb/ptb.test.txt',word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index2word = {v:k for k,v in word2index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module): \n",
    "    def __init__(self,vocab_size,embedding_size,hidden_size,n_layers=1,dropout_p=0.7):\n",
    "\n",
    "        super(LanguageModel, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed = nn.Embedding(vocab_size,embedding_size)\n",
    "        self.rnn = nn.RNN(embedding_size,hidden_size,n_layers,nonlinearity='relu',batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size,vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.init_rnn() # IRNN\n",
    "        self.init_embed()\n",
    "        \n",
    "    def init_embed(self):\n",
    "        self.embed.weight = nn.init.xavier_uniform(self.embed.weight)\n",
    "    \n",
    "    def init_rnn(self):\n",
    "        self.rnn.weight_hh_l0 = nn.init.eye(self.rnn.weight_hh_l0)\n",
    "        self.rnn.weight_ih_l0 = nn.init.eye(self.rnn.weight_ih_l0)\n",
    "        self.rnn.bias_hh_l0.data.fill_(0)\n",
    "        self.rnn.bias_ih_l0.data.fill_(0)\n",
    "        \n",
    "    def init_hidden(self,batch_size):\n",
    "        hidden = Variable(torch.zeros(self.n_layers,batch_size,self.hidden_size))\n",
    "        return hidden.cuda() if USE_CUDA else hidden\n",
    "    \n",
    "    def detach_hidden(self,hidden):\n",
    "        return hidden.detach()\n",
    "    \n",
    "    def forward(self, inputs,hidden,is_training=False): \n",
    "#         hidden = self.init_hidden(inputs)\n",
    "        embeds = self.embed(inputs) # BxWxD\n",
    "        if is_training:\n",
    "            embeds = self.dropout(embeds)\n",
    "        out,hidden = self.rnn(embeds,hidden)\n",
    "        return self.linear(out.contiguous().view(out.size(0)*out.size(1),-1)), hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes for a while. And It sometimes explodes its gradient because of 'relu'. I reference <a href=\"https://arxiv.org/pdf/1504.00941.pdf\">this paper</a> about IRNN. I don't know why it happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBED_SIZE=100\n",
    "HIDDEN_SIZE=512\n",
    "NUM_LAYER=4\n",
    "LR = 0.001\n",
    "SEQ_LENGTH = 30 # for bptt\n",
    "BATCH_SIZE = 20\n",
    "EPOCH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = batchify(train_data,BATCH_SIZE)\n",
    "dev_data = batchify(dev_data,BATCH_SIZE//2)\n",
    "test_data = batchify(test_data,BATCH_SIZE//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = LanguageModel(len(word2index),EMBED_SIZE,HIDDEN_SIZE,NUM_LAYER)\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    losses=[]\n",
    "    hidden = model.init_hidden(BATCH_SIZE)\n",
    "    for i in range(0, train_data.size(1) - SEQ_LENGTH, SEQ_LENGTH):\n",
    "        inputs = Variable(train_data[:, i:i+SEQ_LENGTH])\n",
    "        targets = Variable(train_data[:, (i+1):(i+1)+SEQ_LENGTH].contiguous())\n",
    "        \n",
    "        hidden = model.detach_hidden(hidden)\n",
    "        model.zero_grad()\n",
    "        preds,hidden = model(inputs,hidden,True)\n",
    "\n",
    "        loss = loss_function(preds,targets.view(-1))\n",
    "        losses.append(loss.data.tolist()[0])\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(),0.5) # gradient clipping\n",
    "        optimizer.step()\n",
    "    \n",
    "    if epoch>0:\n",
    "        print(\"[%d/%d] mean_loss : %0.2f, Perplexity : %0.2f\" % (epoch,EPOCH, \\\n",
    "                                                                                              np.mean(losses),np.exp(np.mean(losses))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Perpelexity : 9673.71\n"
     ]
    }
   ],
   "source": [
    "total_loss = 0\n",
    "hidden = model.init_hidden(BATCH_SIZE//2)\n",
    "losses=[]\n",
    "for i in range(0, test_data.size(1) - SEQ_LENGTH, SEQ_LENGTH):\n",
    "    inputs = Variable(test_data[:, i:i+SEQ_LENGTH],volatile=True)\n",
    "    targets = Variable(test_data[:, (i+1):(i+1)+SEQ_LENGTH].contiguous())\n",
    "        \n",
    "    hidden = model.detach_hidden(hidden)\n",
    "    model.zero_grad()\n",
    "    preds,hidden = model(inputs,hidden)\n",
    "\n",
    "    losses.append(loss_function(preds,targets.view(-1)).data.tolist()[0])\n",
    "\n",
    "print(\"Test Perpelexity : %5.2f\" % (np.exp(np.mean(losses))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
