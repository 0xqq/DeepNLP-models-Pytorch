{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Neural Machine Translation and Models with Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture9.pdf\n",
    "* http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture10.pdf\n",
    "* http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture11.pdf\n",
    "* https://arxiv.org/pdf/1409.0473.pdf\n",
    "* http://www.aclweb.org/anthology/P15-1001\n",
    "* https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation-batched.ipynb\n",
    "* http://www.manythings.org/anki/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter, OrderedDict\n",
    "import nltk\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "from torch.nn.utils.rnn import PackedSequence,pack_padded_sequence\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if USE_CUDA else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if USE_CUDA else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if USE_CUDA else torch.ByteTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getBatch(batch_size,train_data):\n",
    "    random.shuffle(train_data)\n",
    "    sindex=0\n",
    "    eindex=batch_size\n",
    "    while eindex < len(train_data):\n",
    "        batch = train_data[sindex:eindex]\n",
    "        temp = eindex\n",
    "        eindex = eindex+batch_size\n",
    "        sindex = temp\n",
    "        yield batch\n",
    "    \n",
    "    if eindex >= len(train_data):\n",
    "        batch = train_data[sindex:]\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_to_batch(batch,x_to_ix,y_to_ix):\n",
    "    \n",
    "    sorted_batch =  sorted(batch, key=lambda b:b[0].size(1),reverse=True) # sort by len\n",
    "    x,y = list(zip(*sorted_batch))\n",
    "    max_x = max([s.size(1) for s in x])\n",
    "    max_y = max([s.size(1) for s in y])\n",
    "    x_p,y_p=[],[]\n",
    "    for i in range(len(batch)):\n",
    "        if x[i].size(1)<max_x:\n",
    "            x_p.append(torch.cat([x[i],Variable(LongTensor([x_to_ix['<PAD>']]*(max_x-x[i].size(1)))).view(1,-1)],1))\n",
    "        else:\n",
    "            x_p.append(x[i])\n",
    "        if y[i].size(1)<max_y:\n",
    "            y_p.append(torch.cat([y[i],Variable(LongTensor([y_to_ix['<PAD>']]*(max_y-y[i].size(1)))).view(1,-1)],1))\n",
    "        else:\n",
    "            y_p.append(y[i])\n",
    "        \n",
    "    input_var = torch.cat(x_p)\n",
    "    target_var = torch.cat(y_p)\n",
    "    input_len = [list(map(lambda s: s ==0, t.data)).count(False) for t in input_var]\n",
    "    target_len = [list(map(lambda s: s ==0, t.data)).count(False) for t in target_var]\n",
    "    \n",
    "    return input_var, target_var, input_len, target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_index):\n",
    "    idxs = list(map(lambda w: to_index[w] if w in to_index.keys() else to_index[\"<UNK>\"], seq))\n",
    "    return Variable(LongTensor(idxs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data load and Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Borrowed code from https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation-batched.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalize_string(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([,.!?])\", r\" \\1 \", s)\n",
    "    s = re.sub(r\"[^a-zA-Z,.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = open('../dataset/eng-fra.txt','r',encoding='utf-8').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142787"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = corpus[:30000] # for practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MIN_LENGTH=3\n",
    "MAX_LENGTH=25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29830 29830\n",
      "['i', 'see', '.'] ['je', 'comprends', '.']\n",
      "CPU times: user 832 ms, sys: 8 ms, total: 840 ms\n",
      "Wall time: 841 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_r,y_r=[],[] # raw\n",
    "\n",
    "for parallel in corpus:\n",
    "    so,ta = parallel[:-1].split('\\t')\n",
    "    if so.strip()==\"\" or ta.strip()==\"\": continue\n",
    "    \n",
    "    normalized_so = normalize_string(so).split()\n",
    "    normalized_ta = normalize_string(ta).split()\n",
    "    \n",
    "    if len(normalized_so)>=MIN_LENGTH and len(normalized_so)<=MAX_LENGTH \\\n",
    "    and len(normalized_ta)>=MIN_LENGTH and len(normalized_ta)<=MAX_LENGTH:\n",
    "        X_r.append(normalized_so)\n",
    "        y_r.append(normalized_ta)\n",
    "    \n",
    "\n",
    "print(len(X_r),len(y_r))\n",
    "print(X_r[0],y_r[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4433 7704\n"
     ]
    }
   ],
   "source": [
    "source_vocab = list(set(flatten(X_r)))\n",
    "target_vocab = list(set(flatten(y_r)))\n",
    "print(len(source_vocab),len(target_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source2index = {'<PAD>':0,'<UNK>':1,'<s>':2,'</s>':3}\n",
    "for vo in source_vocab:\n",
    "    if vo not in source2index.keys():\n",
    "        source2index[vo]=len(source2index)\n",
    "index2source = {v:k for k,v in source2index.items()}\n",
    "\n",
    "target2index = {'<PAD>':0,'<UNK>':1,'<s>':2,'</s>':3}\n",
    "for vo in target_vocab:\n",
    "    if vo not in target2index.keys():\n",
    "        target2index[vo]=len(target2index)\n",
    "index2target = {v:k for k,v in target2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.4 s, sys: 0 ns, total: 1.4 s\n",
      "Wall time: 1.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_p,y_p=[],[]\n",
    "\n",
    "for so,ta in zip(X_r,y_r):\n",
    "    X_p.append(prepare_sequence(so+['</s>'],source2index).view(1,-1))\n",
    "    y_p.append(prepare_sequence(ta+['</s>'],target2index).view(1,-1))\n",
    "    \n",
    "train_data = list(zip(X_p,y_p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size,hidden_size, n_layers=1,bidirec=False):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        \n",
    "        if bidirec:\n",
    "            self.n_direction = 2 \n",
    "            self.gru = nn.GRU(embedding_size, hidden_size, n_layers, batch_first=True,bidirectional=True)\n",
    "        else:\n",
    "            self.n_direction = 1\n",
    "            self.gru = nn.GRU(embedding_size, hidden_size, n_layers, batch_first=True)\n",
    "    \n",
    "    def init_hidden(self,inputs):\n",
    "        hidden = Variable(torch.zeros(self.n_layers*self.n_direction,inputs.size(0),self.hidden_size))\n",
    "        return hidden.cuda() if USE_CUDA else hidden\n",
    "    \n",
    "    def init_weight(self):\n",
    "        self.embedding.weight = nn.init.xavier_uniform(self.embedding.weight)\n",
    "        self.gru.weight_hh_l0 = nn.init.xavier_uniform(self.gru.weight_hh_l0)\n",
    "        self.gru.weight_ih_l0 = nn.init.xavier_uniform(self.gru.weight_ih_l0)\n",
    "    \n",
    "    def forward(self, inputs, input_lengths):\n",
    "        \"\"\"\n",
    "        inputs : B,T (LongTensor)\n",
    "        input_lengths : real lengths of input batch (list)\n",
    "        \"\"\"\n",
    "        hidden = self.init_hidden(inputs)\n",
    "        \n",
    "        embedded = self.embedding(inputs)\n",
    "        packed = pack_padded_sequence(embedded, input_lengths,batch_first=True)\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs,batch_first=True) # unpack (back to padded)\n",
    "                \n",
    "        if self.n_layers>1:\n",
    "            if self.n_direction==2:\n",
    "                hidden = hidden[-2:]\n",
    "            else:\n",
    "                hidden = hidden[-1]\n",
    "        \n",
    "        return outputs, torch.cat(hidden,1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, n_layers=1,dropout_p=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # Define the layers\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        self.gru = nn.GRU(embedding_size+hidden_size, hidden_size, n_layers,batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size*2, input_size)\n",
    "        self.attn = nn.Linear(self.hidden_size,self.hidden_size) # Attention\n",
    "    \n",
    "    def init_hidden(self,inputs):\n",
    "        hidden = Variable(torch.zeros(self.n_layers,inputs.size(0),self.hidden_size))\n",
    "        return hidden.cuda() if USE_CUDA else hidden\n",
    "    \n",
    "    \n",
    "    def init_weight(self):\n",
    "        self.embedding.weight = nn.init.xavier_uniform(self.embedding.weight)\n",
    "        self.gru.weight_hh_l0 = nn.init.xavier_uniform(self.gru.weight_hh_l0)\n",
    "        self.gru.weight_ih_l0 = nn.init.xavier_uniform(self.gru.weight_ih_l0)\n",
    "        self.linear.weight = nn.init.xavier_uniform(self.linear.weight)\n",
    "        self.attn.weight = nn.init.xavier_uniform(self.attn.weight)\n",
    "        self.attn.bias.data.fill_(0)\n",
    "    \n",
    "    def Attention(self, hidden, encoder_outputs, encoder_maskings):\n",
    "        \"\"\"\n",
    "        hidden : 1,B,D\n",
    "        encoder_outputs : B,T,D\n",
    "        encoder_maskings : B,T # ByteTensor\n",
    "        \"\"\"\n",
    "        hidden = hidden[0].unsqueeze(2)  # (1,B,D) -> (B,D,1)\n",
    "        \n",
    "        batch_size = encoder_outputs.size(0) # B\n",
    "        max_len = encoder_outputs.size(1) # T\n",
    "        energies = self.attn(encoder_outputs.contiguous().view(batch_size*max_len,-1)) # B*T,D -> B*T,D\n",
    "        energies = energies.view(batch_size,max_len,-1) # B,T,D\n",
    "        attn_energies = energies.bmm(hidden).squeeze(2) # B,T,D * B,D,1 --> B,T\n",
    "        \n",
    "#         if isinstance(encoder_maskings,torch.autograd.variable.Variable):\n",
    "#             attn_energies = attn_energies.masked_fill(encoder_maskings,float('-inf'))#-1e12) # PAD masking\n",
    "        \n",
    "        alpha = F.softmax(attn_energies) # B,T\n",
    "        alpha = alpha.unsqueeze(1) # B,1,T\n",
    "        context = alpha.bmm(encoder_outputs) # B,1,T * B,T,D => B,1,D\n",
    "        \n",
    "        return context, alpha\n",
    "    \n",
    "    \n",
    "    def forward(self,inputs,context,max_length,encoder_outputs,encoder_maskings=None,is_training=False):\n",
    "        \"\"\"\n",
    "        inputs : B,1 (LongTensor, START SYMBOL)\n",
    "        context : B,1,D (FloatTensor, Last encoder hidden state)\n",
    "        max_length : int, max length to decode # for batch\n",
    "        encoder_outputs : B,T,D\n",
    "        encoder_maskings : B,T # ByteTensor\n",
    "        is_training : bool, this is because adapt dropout only training step.\n",
    "        \"\"\"\n",
    "        # Get the embedding of the current input word\n",
    "        embedded = self.embedding(inputs)\n",
    "        hidden = self.init_hidden(inputs)\n",
    "        if is_training:\n",
    "            embedded = self.dropout(embedded)\n",
    "        \n",
    "        decode=[]\n",
    "        # Apply GRU to the output so far\n",
    "        for i in range(max_length):\n",
    "\n",
    "            _, hidden = self.gru(torch.cat((embedded,context),2), hidden) # h_t = f(h_{t-1},y_{t-1},c)\n",
    "            concated = torch.cat((hidden,context.transpose(0,1)),2) # y_t = g(h_t,y_{t-1},c)\n",
    "            score = self.linear(concated.squeeze(0))\n",
    "            softmaxed = F.log_softmax(score)\n",
    "            decode.append(softmaxed)\n",
    "            decoded = softmaxed.max(1)[1]\n",
    "            embedded = self.embedding(decoded).unsqueeze(1) # y_{t-1}\n",
    "            if is_training:\n",
    "                embedded = self.dropout(embedded)\n",
    "            \n",
    "            # compute next context vector using attention\n",
    "            context, alpha = self.Attention(hidden, encoder_outputs,encoder_maskings)\n",
    "            \n",
    "        #  column-wise concat, reshape!!\n",
    "        scores = torch.cat(decode,1)\n",
    "        return scores.view(inputs.size(0)*max_length,-1)\n",
    "    \n",
    "    def decode(self,context,encoder_outputs):\n",
    "        start_decode = Variable(LongTensor([[target2index['<s>']]*1])).transpose(0,1)\n",
    "        embedded = self.embedding(start_decode)\n",
    "        hidden = self.init_hidden(start_decode)\n",
    "        \n",
    "        decodes=[]\n",
    "        attentions=[]\n",
    "        decoded = embedded\n",
    "        while decoded.data.tolist()[0]!=target2index['</s>']: # until </s>\n",
    "            _, hidden = self.gru(torch.cat((embedded,context),2), hidden) # h_t = f(h_{t-1},y_{t-1},c)\n",
    "            concated = torch.cat((hidden,context.transpose(0,1)),2) # y_t = g(h_t,y_{t-1},c)\n",
    "            score = self.linear(concated.squeeze(0))\n",
    "            softmaxed = F.log_softmax(score)\n",
    "            decodes.append(softmaxed)\n",
    "            decoded = softmaxed.max(1)[1]\n",
    "            embedded = self.embedding(decoded).unsqueeze(1) # y_{t-1}\n",
    "            context, alpha = self.Attention(hidden, encoder_outputs,None)\n",
    "            attentions.append(alpha.squeeze(1))\n",
    "        \n",
    "        return torch.cat(decodes).max(1)[1], torch.cat(attentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes for a while..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPOCH=50\n",
    "BATCH_SIZE = 64\n",
    "EMBEDDING_SIZE = 100\n",
    "HIDDEN_SIZE = 256\n",
    "LR = 0.0001\n",
    "DECODER_LEARNING_RATIO=5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(len(source2index),EMBEDDING_SIZE,HIDDEN_SIZE,3,True)\n",
    "decoder = Decoder(len(target2index),EMBEDDING_SIZE,HIDDEN_SIZE*2)\n",
    "encoder.init_weight()\n",
    "decoder.init_weight()\n",
    "\n",
    "if USE_CUDA:\n",
    "    encoder = encoder.cuda()\n",
    "    decoder = decoder.cuda()\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
    "enc_optimizer = optim.Adam(encoder.parameters(),lr=LR)\n",
    "dec_optimizer = optim.Adam(decoder.parameters(),lr=LR*DECODER_LEARNING_RATIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/50] [0000/466] mean_loss : 0.36\n",
      "[00/50] [0100/466] mean_loss : 0.32\n",
      "[00/50] [0200/466] mean_loss : 0.33\n",
      "[00/50] [0300/466] mean_loss : 0.33\n",
      "[00/50] [0400/466] mean_loss : 0.33\n",
      "[01/50] [0000/466] mean_loss : 0.34\n",
      "[01/50] [0100/466] mean_loss : 0.32\n",
      "[01/50] [0200/466] mean_loss : 0.32\n",
      "[01/50] [0300/466] mean_loss : 0.32\n",
      "[01/50] [0400/466] mean_loss : 0.33\n",
      "[02/50] [0000/466] mean_loss : 0.26\n",
      "[02/50] [0100/466] mean_loss : 0.32\n",
      "[02/50] [0200/466] mean_loss : 0.32\n",
      "[02/50] [0300/466] mean_loss : 0.32\n",
      "[02/50] [0400/466] mean_loss : 0.32\n",
      "[03/50] [0000/466] mean_loss : 0.26\n",
      "[03/50] [0100/466] mean_loss : 0.31\n",
      "[03/50] [0200/466] mean_loss : 0.31\n",
      "[03/50] [0300/466] mean_loss : 0.32\n",
      "[03/50] [0400/466] mean_loss : 0.32\n",
      "[04/50] [0000/466] mean_loss : 0.29\n",
      "[04/50] [0100/466] mean_loss : 0.30\n",
      "[04/50] [0200/466] mean_loss : 0.31\n",
      "[04/50] [0300/466] mean_loss : 0.31\n",
      "[04/50] [0400/466] mean_loss : 0.32\n",
      "[05/50] [0000/466] mean_loss : 0.26\n",
      "[05/50] [0100/466] mean_loss : 0.30\n",
      "[05/50] [0200/466] mean_loss : 0.31\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-581-58b28e14b93e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mlosses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_lengths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_to_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msource2index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget2index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0minput_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    losses=[]\n",
    "    for i,batch in enumerate(getBatch(BATCH_SIZE,train_data)):\n",
    "        inputs,targets,input_lengths,target_lengths = pad_to_batch(batch,source2index,target2index)\n",
    "        \n",
    "        input_masks = torch.cat([Variable(ByteTensor(tuple(map(lambda s: s ==0 or s==3, t.data)))) for t in inputs]).view(inputs.size(0),-1)\n",
    "        start_decode = Variable(LongTensor([[target2index['<s>']]*targets.size(0)])).transpose(0,1)\n",
    "        #smaller_vocab = uniform_candidate_sampler(targets,200,len(word2index))\n",
    "        encoder.zero_grad()\n",
    "        decoder.zero_grad()\n",
    "        output, hidden_c = encoder(inputs,input_lengths)\n",
    "        \n",
    "        preds = decoder(start_decode,hidden_c,targets.size(1),output,input_masks,True)\n",
    "                                \n",
    "        loss = loss_function(preds,targets.view(-1))\n",
    "        losses.append(loss.data.tolist()[0] )\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(encoder.parameters(), 50.0) # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm(decoder.parameters(), 50.0) # gradient clipping\n",
    "        enc_optimizer.step()\n",
    "        dec_optimizer.step()\n",
    "\n",
    "    \n",
    "        if i % 100==0:\n",
    "            print(\"[%02d/%d] [%04d/%d] mean_loss : %0.2f\" %(epoch,EPOCH,i,len(train_data)//BATCH_SIZE,np.mean(losses)))\n",
    "            losses=[]\n",
    "#     torch.save(decoder.state_dict(),'../model/decoder.pkl')\n",
    "#     torch.save(encoder.state_dict(),'../model/', 'encoder.pkl')\n",
    "\n",
    "\n",
    "#     if (step+1) % 10 == 0:\n",
    "#         LR = LR/2\n",
    "#         enc_optimizer = optim.Adam(encoder.parameters(),lr=LR)\n",
    "#         dec_optimizer = optim.Adam(decoder.parameters(),lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# borrowed code from https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation-batched.ipynb\n",
    "\n",
    "def show_attention(input_words, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_words, rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "#     show_plot_visdom()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source :  he has a foreign car .\n",
      "Truth :  il possede une voiture etrangere .\n",
      "Prediction :  il possede une voiture etrangere .\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVIAAAEYCAYAAAAOFn7lAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ne2jEygAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f817f10c390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = random.choice(train_data)\n",
    "input_ = test[0]\n",
    "truth = test[1]\n",
    "# start_decode = Variable(LongTensor([[target2index['<s>']]*1])).transpose(0,1)\n",
    "\n",
    "output, hidden = encoder(input_,[input_.size(1)])\n",
    "pred,attn = decoder.decode(hidden,output)\n",
    "\n",
    "input_ = [index2source[i] for i in input_.data.tolist()[0]]\n",
    "pred = [index2target[i] for i in pred.data.tolist()]\n",
    "\n",
    "\n",
    "print('Source : ',' '.join([i for i in input_ if i not in ['</s>']]))\n",
    "print('Truth : ',' '.join([index2target[i] for i in truth.data.tolist()[0] if i not in [2,3]]))\n",
    "print('Prediction : ',' '.join([i for i in pred if i not in ['</s>']]))\n",
    "\n",
    "if USE_CUDA:\n",
    "    attn = attn.cpu()\n",
    "\n",
    "show_attention(input_,pred,attn.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* BLEU\n",
    "* Beam Search\n",
    "* Sampled Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
