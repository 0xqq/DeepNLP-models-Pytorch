{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Neural Machine Translation and Models with Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture9.pdf\n",
    "* http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture10.pdf\n",
    "* http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture11.pdf\n",
    "* https://arxiv.org/pdf/1409.0473.pdf\n",
    "* http://www.aclweb.org/anthology/P15-1001\n",
    "* https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation-batched.ipynb\n",
    "* http://www.manythings.org/anki/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter, OrderedDict\n",
    "import nltk\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "from torch.nn.utils.rnn import PackedSequence,pack_padded_sequence\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if USE_CUDA else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if USE_CUDA else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if USE_CUDA else torch.ByteTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getBatch(batch_size,train_data):\n",
    "    random.shuffle(train_data)\n",
    "    sindex=0\n",
    "    eindex=batch_size\n",
    "    while eindex < len(train_data):\n",
    "        batch = train_data[sindex:eindex]\n",
    "        temp = eindex\n",
    "        eindex = eindex+batch_size\n",
    "        sindex = temp\n",
    "        yield batch\n",
    "    \n",
    "    if eindex >= len(train_data):\n",
    "        batch = train_data[sindex:]\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_to_batch(batch,x_to_ix,y_to_ix):\n",
    "    \n",
    "    sorted_batch =  sorted(batch, key=lambda b:b[0].size(1),reverse=True) # sort by len\n",
    "    x,y = list(zip(*sorted_batch))\n",
    "    max_x = max([s.size(1) for s in x])\n",
    "    max_y = max([s.size(1) for s in y])\n",
    "    x_p,y_p=[],[]\n",
    "    for i in range(len(batch)):\n",
    "        if x[i].size(1)<max_x:\n",
    "            x_p.append(torch.cat([x[i],Variable(LongTensor([x_to_ix['<PAD>']]*(max_x-x[i].size(1)))).view(1,-1)],1))\n",
    "        else:\n",
    "            x_p.append(x[i])\n",
    "        if y[i].size(1)<max_y:\n",
    "            y_p.append(torch.cat([y[i],Variable(LongTensor([y_to_ix['<PAD>']]*(max_y-y[i].size(1)))).view(1,-1)],1))\n",
    "        else:\n",
    "            y_p.append(y[i])\n",
    "        \n",
    "    input_var = torch.cat(x_p)\n",
    "    target_var = torch.cat(y_p)\n",
    "    input_len = [list(map(lambda s: s ==0, t.data)).count(False) for t in input_var]\n",
    "    target_len = [list(map(lambda s: s ==0, t.data)).count(False) for t in target_var]\n",
    "    \n",
    "    return input_var, target_var, input_len, target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_index):\n",
    "    idxs = list(map(lambda w: to_index[w] if w in to_index.keys() else to_index[\"<UNK>\"], seq))\n",
    "    return Variable(LongTensor(idxs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data load and Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Borrowed code from https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation-batched.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalize_string(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([,.!?])\", r\" \\1 \", s)\n",
    "    s = re.sub(r\"[^a-zA-Z,.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = open('../dataset/eng-fra.txt','r',encoding='utf-8').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MIN_LENGTH=3\n",
    "MAX_LENGTH=25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142474 142474\n",
      "['i', 'see', '.'] ['je', 'comprends', '.']\n",
      "CPU times: user 5.04 s, sys: 12 ms, total: 5.05 s\n",
      "Wall time: 5.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_r,y_r=[],[] # raw\n",
    "\n",
    "for parallel in corpus:\n",
    "    so,ta = parallel[:-1].split('\\t')\n",
    "    if so.strip()==\"\" or ta.strip()==\"\": continue\n",
    "    \n",
    "    normalized_so = normalize_string(so).split()\n",
    "    normalized_ta = normalize_string(ta).split()\n",
    "    \n",
    "    if len(normalized_so)>=MIN_LENGTH and len(normalized_so)<=MAX_LENGTH \\\n",
    "    and len(normalized_ta)>=MIN_LENGTH and len(normalized_ta)<=MAX_LENGTH:\n",
    "        X_r.append(normalized_so)\n",
    "        y_r.append(normalized_ta)\n",
    "    \n",
    "\n",
    "print(len(X_r),len(y_r))\n",
    "print(X_r[0],y_r[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13102 21510\n"
     ]
    }
   ],
   "source": [
    "source_vocab = list(set(flatten(X_r)))\n",
    "target_vocab = list(set(flatten(y_r)))\n",
    "print(len(source_vocab),len(target_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source2index = {'<PAD>':0,'<UNK>':1,'<s>':2,'</s>':3}\n",
    "for vo in source_vocab:\n",
    "    if vo not in source2index.keys():\n",
    "        source2index[vo]=len(source2index)\n",
    "index2source = {v:k for k,v in source2index.items()}\n",
    "\n",
    "target2index = {'<PAD>':0,'<UNK>':1,'<s>':2,'</s>':3}\n",
    "for vo in target_vocab:\n",
    "    if vo not in target2index.keys():\n",
    "        target2index[vo]=len(target2index)\n",
    "index2target = {v:k for k,v in target2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.76 s, sys: 308 ms, total: 7.06 s\n",
      "Wall time: 7.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_p,y_p=[],[]\n",
    "\n",
    "for so,ta in zip(X_r,y_r):\n",
    "    X_p.append(prepare_sequence(['<s>']+so+['</s>'],source2index).view(1,-1))\n",
    "    y_p.append(prepare_sequence(ta+['</s>'],target2index).view(1,-1))\n",
    "    \n",
    "train_data = list(zip(X_p,y_p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size,hidden_size, n_layers=1,bidirec=False):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        \n",
    "        if bidirec:\n",
    "            self.n_direction = 2 \n",
    "            self.gru = nn.GRU(embedding_size, hidden_size, n_layers, batch_first=True,bidirectional=True)\n",
    "        else:\n",
    "            self.n_direction = 1\n",
    "            self.gru = nn.GRU(embedding_size, hidden_size, n_layers, batch_first=True)\n",
    "    \n",
    "    def init_hidden(self,inputs):\n",
    "        hidden = Variable(torch.zeros(self.n_layers*self.n_direction,inputs.size(0),self.hidden_size))\n",
    "        return hidden.cuda() if USE_CUDA else hidden\n",
    "    \n",
    "    def init_weight(self):\n",
    "        self.embedding.weight = nn.init.xavier_uniform(self.embedding.weight)\n",
    "#         self.gru.weight_hh_l0 = nn.init.xavier_uniform(self.gru.weight_hh_l0)\n",
    "#         self.gru.weight_ih_l0 = nn.init.xavier_uniform(self.gru.weight_ih_l0)\n",
    "    \n",
    "    def forward(self, inputs, input_lengths):\n",
    "        \"\"\"\n",
    "        inputs : B,T (LongTensor)\n",
    "        input_lengths : real lengths of input batch (list)\n",
    "        \"\"\"\n",
    "        hidden = self.init_hidden(inputs)\n",
    "        \n",
    "        embedded = self.embedding(inputs)\n",
    "        packed = pack_padded_sequence(embedded, input_lengths,batch_first=True)\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs,batch_first=True) # unpack (back to padded)\n",
    "        \n",
    "        if self.n_layers>1:\n",
    "            if self.n_direction==2:\n",
    "                hidden = hidden[-2:]\n",
    "            else:\n",
    "                hidden = hidden[-1]\n",
    "        \n",
    "        return outputs, torch.cat(hidden,1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, n_layers=1,dropout_p=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # Define the layers\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        self.gru = nn.GRU(embedding_size+hidden_size, hidden_size, n_layers,batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size*2, input_size)\n",
    "        self.attn = nn.Linear(self.hidden_size,self.hidden_size) # Attention\n",
    "        \n",
    "    def init_hidden(self,inputs):\n",
    "        hidden = Variable(torch.zeros(self.n_layers,inputs.size(0),self.hidden_size))\n",
    "        return hidden.cuda() if USE_CUDA else hidden\n",
    "    \n",
    "    \n",
    "    def init_weight(self):\n",
    "        self.embedding.weight = nn.init.xavier_uniform(self.embedding.weight)\n",
    "#         self.gru.weight_hh_l0 = nn.init.xavier_uniform(self.gru.weight_hh_l0)\n",
    "#         self.gru.weight_ih_l0 = nn.init.xavier_uniform(self.gru.weight_ih_l0)\n",
    "        self.linear.weight = nn.init.xavier_uniform(self.linear.weight)\n",
    "        self.attn.weight = nn.init.xavier_uniform(self.attn.weight)\n",
    "        self.attn.bias.data.fill_(0)\n",
    "    \n",
    "    def Attention(self, hidden, encoder_outputs, encoder_maskings):\n",
    "        \"\"\"\n",
    "        hidden : 1,B,D\n",
    "        encoder_outputs : B,T,D\n",
    "        encoder_maskings : B,T # ByteTensor\n",
    "        \"\"\"\n",
    "        \n",
    "        hidden = hidden.squeeze(0).unsqueeze(2)  # 히든 : (1,B,D) -> (B,D,1)\n",
    "        \n",
    "        batch_size = encoder_outputs.size(0) # B\n",
    "        max_len = encoder_outputs.size(1) # T\n",
    "        energies = self.attn(encoder_outputs.contiguous().view(batch_size*max_len,-1)) # B*T,D -> B*T,D\n",
    "        energies = energies.view(batch_size,max_len,-1) # B,T,D\n",
    "        attn_energies = energies.bmm(hidden).transpose(1,2).squeeze(1) # B,T,D * B,D,1 --> B,1,T\n",
    "        \n",
    "        if isinstance(encoder_maskings,torch.autograd.variable.Variable):\n",
    "            attn_energies = attn_energies.masked_fill(encoder_maskings,-1e12) # PAD masking\n",
    "\n",
    "        alpha = F.softmax(attn_energies) # B,T\n",
    "        alpha = alpha.unsqueeze(1) # B,1,T\n",
    "        context = alpha.bmm(encoder_outputs) # B,1,T * B,T,D => B,1,D\n",
    "        \n",
    "        return context, alpha\n",
    "    \n",
    "    def decode(self,context,encoder_outputs):\n",
    "        start_decode = Variable(LongTensor([[target2index['<s>']]*1])).transpose(0,1)\n",
    "        embedded = self.embedding(start_decode)\n",
    "        hidden = self.init_hidden(start_decode)\n",
    "        \n",
    "        decodes=[]\n",
    "        attentions=[]\n",
    "        decoded = inputs\n",
    "        while decoded.data.tolist()[0]!=target2index['</s>']: # until </s>\n",
    "            _, hidden = self.gru(torch.cat((embedded,context),2), hidden) # h_t = f(h_{t-1},y_{t-1},c)\n",
    "            concated = torch.cat((hidden,context.transpose(0,1)),2) # y_t = g(h_t,y_{t-1},c)\n",
    "            score = self.linear(concated.squeeze(0))\n",
    "            softmaxed = F.log_softmax(score)\n",
    "            decodes.append(softmaxed)\n",
    "            decoded = softmaxed.max(1)[1]\n",
    "            embedded = self.embedding(decoded).unsqueeze(1) # y_{t-1}\n",
    "            context, alpha = self.Attention(hidden, encoder_outputs,None) \n",
    "            attentions.append(alpha.squeeze(0))\n",
    "        \n",
    "        return torch.cat(decodes).max(1)[1], torch.cat(attentions)\n",
    "    \n",
    "    def forward(self,inputs,context,max_length,encoder_outputs,encoder_maskings=None,is_training=False):\n",
    "        \"\"\"\n",
    "        inputs : B,1 (LongTensor, START SYMBOL)\n",
    "        context : B,1,D (FloatTensor, Last encoder hidden state)\n",
    "        max_length : int, max length to decode\n",
    "        encoder_outputs : B,T,D\n",
    "        encoder_maskings : B,T # ByteTensor\n",
    "        is_training : bool, this is because adapt dropout only training step.\n",
    "        \"\"\"\n",
    "        # Get the embedding of the current input word\n",
    "        embedded = self.embedding(inputs)\n",
    "        hidden = self.init_hidden(inputs)\n",
    "        if is_training:\n",
    "            embedded = self.dropout(embedded)\n",
    "        \n",
    "        decode=[]\n",
    "        # Apply GRU to the output so far\n",
    "        for i in range(max_length):\n",
    "\n",
    "            _, hidden = self.gru(torch.cat((embedded,context),2), hidden) # h_t = f(h_{t-1},y_{t-1},c)\n",
    "            concated = torch.cat((hidden,context.transpose(0,1)),2) # y_t = g(h_t,y_{t-1},c)\n",
    "            score = self.linear(concated.squeeze(0))\n",
    "            softmaxed = F.log_softmax(score)\n",
    "            decode.append(softmaxed)\n",
    "            decoded = softmaxed.max(1)[1]\n",
    "            embedded = self.embedding(decoded).unsqueeze(1) # y_{t-1}\n",
    "            if is_training:\n",
    "                embedded = self.dropout(embedded)\n",
    "            \n",
    "            # compute next context vector using attention\n",
    "            context, alpha = self.Attention(hidden, encoder_outputs,encoder_maskings) \n",
    "        #  column-wise concat, reshape!!\n",
    "        scores = torch.cat(decode,1)\n",
    "        return scores.view(inputs.size(0)*max_length,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes for a while..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPOCH=50\n",
    "BATCH_SIZE = 64\n",
    "EMBEDDING_SIZE = 100\n",
    "HIDDEN_SIZE = 256\n",
    "LR = 0.0001\n",
    "DECODER_LEARNING_RATIO=5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(len(source2index),EMBEDDING_SIZE,HIDDEN_SIZE,3,True)\n",
    "decoder = Decoder(len(target2index),EMBEDDING_SIZE,HIDDEN_SIZE*2)\n",
    "encoder.init_weight()\n",
    "decoder.init_weight()\n",
    "\n",
    "if USE_CUDA:\n",
    "    encoder = encoder.cuda()\n",
    "    decoder = decoder.cuda()\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
    "enc_optimizer = optim.Adam(encoder.parameters(),lr=LR)\n",
    "dec_optimizer = optim.Adam(decoder.parameters(),lr=LR*DECODER_LEARNING_RATIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoder.load_state_dict(torch.load('../model/tencoder.pkl'))\n",
    "decoder.load_state_dict(torch.load('../model/tdecoder.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.save(decoder.state_dict(),'../model/tdecoder.pkl')\n",
    "torch.save(encoder.state_dict(),'../model/tencoder.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/50] [0000/2226] mean_loss : 9.98\n",
      "[00/50] [0500/2226] mean_loss : 5.47\n",
      "[00/50] [1000/2226] mean_loss : 4.89\n",
      "[00/50] [1500/2226] mean_loss : 4.59\n",
      "[00/50] [2000/2226] mean_loss : 4.38\n",
      "[01/50] [0000/2226] mean_loss : 4.17\n",
      "[01/50] [0500/2226] mean_loss : 4.09\n",
      "[01/50] [1000/2226] mean_loss : 4.02\n",
      "[01/50] [1500/2226] mean_loss : 3.92\n",
      "[01/50] [2000/2226] mean_loss : 3.82\n",
      "[02/50] [0000/2226] mean_loss : 3.76\n",
      "[02/50] [0500/2226] mean_loss : 3.60\n",
      "[02/50] [1000/2226] mean_loss : 3.55\n",
      "[02/50] [1500/2226] mean_loss : 3.51\n",
      "[02/50] [2000/2226] mean_loss : 3.46\n",
      "[03/50] [0000/2226] mean_loss : 3.40\n",
      "[03/50] [0500/2226] mean_loss : 3.22\n",
      "[03/50] [1000/2226] mean_loss : 3.20\n",
      "[03/50] [1500/2226] mean_loss : 3.19\n",
      "[03/50] [2000/2226] mean_loss : 3.16\n",
      "[04/50] [0000/2226] mean_loss : 2.89\n",
      "[04/50] [0500/2226] mean_loss : 2.91\n",
      "[04/50] [1000/2226] mean_loss : 2.91\n",
      "[04/50] [1500/2226] mean_loss : 2.90\n",
      "[04/50] [2000/2226] mean_loss : 2.87\n",
      "[05/50] [0000/2226] mean_loss : 2.62\n",
      "[05/50] [0500/2226] mean_loss : 2.63\n",
      "[05/50] [1000/2226] mean_loss : 2.66\n",
      "[05/50] [1500/2226] mean_loss : 2.65\n",
      "[05/50] [2000/2226] mean_loss : 2.65\n",
      "[06/50] [0000/2226] mean_loss : 2.26\n",
      "[06/50] [0500/2226] mean_loss : 2.41\n",
      "[06/50] [1000/2226] mean_loss : 2.44\n",
      "[06/50] [1500/2226] mean_loss : 2.44\n",
      "[06/50] [2000/2226] mean_loss : 2.45\n",
      "[07/50] [0000/2226] mean_loss : 2.10\n",
      "[07/50] [0500/2226] mean_loss : 2.23\n",
      "[07/50] [1000/2226] mean_loss : 2.23\n",
      "[07/50] [1500/2226] mean_loss : 2.27\n",
      "[07/50] [2000/2226] mean_loss : 2.28\n",
      "[08/50] [0000/2226] mean_loss : 2.11\n",
      "[08/50] [0500/2226] mean_loss : 2.05\n",
      "[08/50] [1000/2226] mean_loss : 2.09\n",
      "[08/50] [1500/2226] mean_loss : 2.12\n",
      "[08/50] [2000/2226] mean_loss : 2.12\n",
      "[09/50] [0000/2226] mean_loss : 2.07\n",
      "[09/50] [0500/2226] mean_loss : 1.93\n",
      "[09/50] [1000/2226] mean_loss : 1.96\n",
      "[09/50] [1500/2226] mean_loss : 1.99\n",
      "[09/50] [2000/2226] mean_loss : 2.00\n",
      "[10/50] [0000/2226] mean_loss : 1.72\n",
      "[10/50] [0500/2226] mean_loss : 1.80\n",
      "[10/50] [1000/2226] mean_loss : 1.84\n",
      "[10/50] [1500/2226] mean_loss : 1.86\n",
      "[10/50] [2000/2226] mean_loss : 1.89\n",
      "[11/50] [0000/2226] mean_loss : 1.70\n",
      "[11/50] [0500/2226] mean_loss : 1.70\n",
      "[11/50] [1000/2226] mean_loss : 1.73\n",
      "[11/50] [1500/2226] mean_loss : 1.76\n",
      "[11/50] [2000/2226] mean_loss : 1.79\n",
      "[12/50] [0000/2226] mean_loss : 1.50\n",
      "[12/50] [0500/2226] mean_loss : 1.60\n",
      "[12/50] [1000/2226] mean_loss : 1.63\n",
      "[12/50] [1500/2226] mean_loss : 1.68\n",
      "[12/50] [2000/2226] mean_loss : 1.71\n",
      "[13/50] [0000/2226] mean_loss : 1.31\n",
      "[13/50] [0500/2226] mean_loss : 1.51\n",
      "[13/50] [1000/2226] mean_loss : 1.57\n",
      "[13/50] [1500/2226] mean_loss : 1.60\n",
      "[13/50] [2000/2226] mean_loss : 1.62\n",
      "[14/50] [0000/2226] mean_loss : 1.26\n",
      "[14/50] [0500/2226] mean_loss : 1.44\n",
      "[14/50] [1000/2226] mean_loss : 1.50\n",
      "[14/50] [1500/2226] mean_loss : 1.53\n",
      "[14/50] [2000/2226] mean_loss : 1.55\n",
      "[15/50] [0000/2226] mean_loss : 1.37\n",
      "[15/50] [0500/2226] mean_loss : 1.38\n",
      "[15/50] [1000/2226] mean_loss : 1.43\n",
      "[15/50] [1500/2226] mean_loss : 1.47\n",
      "[15/50] [2000/2226] mean_loss : 1.49\n",
      "[16/50] [0000/2226] mean_loss : 1.31\n",
      "[16/50] [0500/2226] mean_loss : 1.32\n",
      "[16/50] [1000/2226] mean_loss : 1.38\n",
      "[16/50] [1500/2226] mean_loss : 1.41\n",
      "[16/50] [2000/2226] mean_loss : 1.44\n",
      "[17/50] [0000/2226] mean_loss : 1.04\n",
      "[17/50] [0500/2226] mean_loss : 1.27\n",
      "[17/50] [1000/2226] mean_loss : 1.32\n",
      "[17/50] [1500/2226] mean_loss : 1.37\n",
      "[17/50] [2000/2226] mean_loss : 1.38\n",
      "[18/50] [0000/2226] mean_loss : 0.99\n",
      "[18/50] [0500/2226] mean_loss : 1.23\n",
      "[18/50] [1000/2226] mean_loss : 1.28\n",
      "[18/50] [1500/2226] mean_loss : 1.32\n",
      "[18/50] [2000/2226] mean_loss : 1.33\n",
      "[19/50] [0000/2226] mean_loss : 1.04\n",
      "[19/50] [0500/2226] mean_loss : 1.19\n",
      "[19/50] [1000/2226] mean_loss : 1.23\n",
      "[19/50] [1500/2226] mean_loss : 1.26\n",
      "[19/50] [2000/2226] mean_loss : 1.30\n",
      "[20/50] [0000/2226] mean_loss : 1.21\n",
      "[20/50] [0500/2226] mean_loss : 1.14\n",
      "[20/50] [1000/2226] mean_loss : 1.20\n",
      "[20/50] [1500/2226] mean_loss : 1.23\n",
      "[20/50] [2000/2226] mean_loss : 1.26\n",
      "[21/50] [0000/2226] mean_loss : 1.26\n",
      "[21/50] [0500/2226] mean_loss : 1.11\n",
      "[21/50] [1000/2226] mean_loss : 1.16\n",
      "[21/50] [1500/2226] mean_loss : 1.20\n",
      "[21/50] [2000/2226] mean_loss : 1.22\n",
      "[22/50] [0000/2226] mean_loss : 1.16\n",
      "[22/50] [0500/2226] mean_loss : 1.08\n",
      "[22/50] [1000/2226] mean_loss : 1.12\n",
      "[22/50] [1500/2226] mean_loss : 1.15\n",
      "[22/50] [2000/2226] mean_loss : 1.19\n",
      "[23/50] [0000/2226] mean_loss : 1.04\n",
      "[23/50] [0500/2226] mean_loss : 1.04\n",
      "[23/50] [1000/2226] mean_loss : 1.10\n",
      "[23/50] [1500/2226] mean_loss : 1.13\n",
      "[23/50] [2000/2226] mean_loss : 1.16\n",
      "[24/50] [0000/2226] mean_loss : 1.02\n",
      "[24/50] [0500/2226] mean_loss : 1.01\n",
      "[24/50] [1000/2226] mean_loss : 1.07\n",
      "[24/50] [1500/2226] mean_loss : 1.11\n",
      "[24/50] [2000/2226] mean_loss : 1.13\n",
      "[25/50] [0000/2226] mean_loss : 0.98\n",
      "[25/50] [0500/2226] mean_loss : 0.99\n",
      "[25/50] [1000/2226] mean_loss : 1.04\n",
      "[25/50] [1500/2226] mean_loss : 1.08\n",
      "[25/50] [2000/2226] mean_loss : 1.10\n",
      "[26/50] [0000/2226] mean_loss : 0.90\n",
      "[26/50] [0500/2226] mean_loss : 0.97\n",
      "[26/50] [1000/2226] mean_loss : 1.02\n",
      "[26/50] [1500/2226] mean_loss : 1.05\n",
      "[26/50] [2000/2226] mean_loss : 1.08\n",
      "[27/50] [0000/2226] mean_loss : 0.89\n",
      "[27/50] [0500/2226] mean_loss : 0.95\n",
      "[27/50] [1000/2226] mean_loss : 0.99\n",
      "[27/50] [1500/2226] mean_loss : 1.03\n",
      "[27/50] [2000/2226] mean_loss : 1.06\n",
      "[28/50] [0000/2226] mean_loss : 0.89\n",
      "[28/50] [0500/2226] mean_loss : 0.92\n",
      "[28/50] [1000/2226] mean_loss : 0.97\n",
      "[28/50] [1500/2226] mean_loss : 1.01\n",
      "[28/50] [2000/2226] mean_loss : 1.03\n",
      "[29/50] [0000/2226] mean_loss : 0.78\n",
      "[29/50] [0500/2226] mean_loss : 0.91\n",
      "[29/50] [1000/2226] mean_loss : 0.95\n",
      "[29/50] [1500/2226] mean_loss : 0.99\n",
      "[29/50] [2000/2226] mean_loss : 1.02\n",
      "[30/50] [0000/2226] mean_loss : 0.80\n",
      "[30/50] [0500/2226] mean_loss : 0.89\n",
      "[30/50] [1000/2226] mean_loss : 0.94\n",
      "[30/50] [1500/2226] mean_loss : 0.97\n",
      "[30/50] [2000/2226] mean_loss : 0.99\n",
      "[31/50] [0000/2226] mean_loss : 0.95\n",
      "[31/50] [0500/2226] mean_loss : 0.87\n",
      "[31/50] [1000/2226] mean_loss : 0.91\n",
      "[31/50] [1500/2226] mean_loss : 0.95\n",
      "[31/50] [2000/2226] mean_loss : 0.97\n",
      "[32/50] [0000/2226] mean_loss : 0.75\n",
      "[32/50] [0500/2226] mean_loss : 0.85\n",
      "[32/50] [1000/2226] mean_loss : 0.90\n",
      "[32/50] [1500/2226] mean_loss : 0.93\n",
      "[32/50] [2000/2226] mean_loss : 0.96\n",
      "[33/50] [0000/2226] mean_loss : 0.77\n",
      "[33/50] [0500/2226] mean_loss : 0.83\n",
      "[33/50] [1000/2226] mean_loss : 0.89\n",
      "[33/50] [1500/2226] mean_loss : 0.92\n",
      "[33/50] [2000/2226] mean_loss : 0.95\n",
      "[34/50] [0000/2226] mean_loss : 0.89\n",
      "[34/50] [0500/2226] mean_loss : 0.82\n",
      "[34/50] [1000/2226] mean_loss : 0.87\n",
      "[34/50] [1500/2226] mean_loss : 0.90\n",
      "[34/50] [2000/2226] mean_loss : 0.93\n",
      "[35/50] [0000/2226] mean_loss : 0.87\n",
      "[35/50] [0500/2226] mean_loss : 0.81\n",
      "[35/50] [1000/2226] mean_loss : 0.86\n",
      "[35/50] [1500/2226] mean_loss : 0.89\n",
      "[35/50] [2000/2226] mean_loss : 0.92\n",
      "[36/50] [0000/2226] mean_loss : 0.76\n",
      "[36/50] [0500/2226] mean_loss : 0.80\n",
      "[36/50] [1000/2226] mean_loss : 0.84\n",
      "[36/50] [1500/2226] mean_loss : 0.88\n",
      "[36/50] [2000/2226] mean_loss : 0.90\n",
      "[37/50] [0000/2226] mean_loss : 0.74\n",
      "[37/50] [0500/2226] mean_loss : 0.78\n",
      "[37/50] [1000/2226] mean_loss : 0.83\n",
      "[37/50] [1500/2226] mean_loss : 0.86\n",
      "[37/50] [2000/2226] mean_loss : 0.89\n",
      "[38/50] [0000/2226] mean_loss : 0.71\n",
      "[38/50] [0500/2226] mean_loss : 0.77\n",
      "[38/50] [1000/2226] mean_loss : 0.82\n",
      "[38/50] [1500/2226] mean_loss : 0.85\n",
      "[38/50] [2000/2226] mean_loss : 0.88\n",
      "[39/50] [0000/2226] mean_loss : 0.66\n",
      "[39/50] [0500/2226] mean_loss : 0.76\n",
      "[39/50] [1000/2226] mean_loss : 0.82\n",
      "[39/50] [1500/2226] mean_loss : 0.83\n",
      "[39/50] [2000/2226] mean_loss : 0.86\n",
      "[40/50] [0000/2226] mean_loss : 0.70\n",
      "[40/50] [0500/2226] mean_loss : 0.75\n",
      "[40/50] [1000/2226] mean_loss : 0.79\n",
      "[40/50] [1500/2226] mean_loss : 0.83\n",
      "[40/50] [2000/2226] mean_loss : 0.85\n",
      "[41/50] [0000/2226] mean_loss : 0.60\n",
      "[41/50] [0500/2226] mean_loss : 0.74\n",
      "[41/50] [1000/2226] mean_loss : 0.79\n",
      "[41/50] [1500/2226] mean_loss : 0.82\n",
      "[41/50] [2000/2226] mean_loss : 0.85\n",
      "[42/50] [0000/2226] mean_loss : 0.60\n",
      "[42/50] [0500/2226] mean_loss : 0.73\n",
      "[42/50] [1000/2226] mean_loss : 0.78\n",
      "[42/50] [1500/2226] mean_loss : 0.81\n",
      "[42/50] [2000/2226] mean_loss : 0.84\n",
      "[43/50] [0000/2226] mean_loss : 0.68\n",
      "[43/50] [0500/2226] mean_loss : 0.72\n",
      "[43/50] [1000/2226] mean_loss : 0.77\n",
      "[43/50] [1500/2226] mean_loss : 0.80\n",
      "[43/50] [2000/2226] mean_loss : 0.82\n",
      "[44/50] [0000/2226] mean_loss : 0.77\n",
      "[44/50] [0500/2226] mean_loss : 0.72\n",
      "[44/50] [1000/2226] mean_loss : 0.76\n",
      "[44/50] [1500/2226] mean_loss : 0.79\n",
      "[44/50] [2000/2226] mean_loss : 0.82\n",
      "[45/50] [0000/2226] mean_loss : 0.82\n",
      "[45/50] [0500/2226] mean_loss : 0.71\n",
      "[45/50] [1000/2226] mean_loss : 0.75\n",
      "[45/50] [1500/2226] mean_loss : 0.79\n",
      "[45/50] [2000/2226] mean_loss : 0.81\n",
      "[46/50] [0000/2226] mean_loss : 0.63\n",
      "[46/50] [0500/2226] mean_loss : 0.70\n",
      "[46/50] [1000/2226] mean_loss : 0.74\n",
      "[46/50] [1500/2226] mean_loss : 0.77\n",
      "[46/50] [2000/2226] mean_loss : 0.80\n",
      "[47/50] [0000/2226] mean_loss : 0.67\n",
      "[47/50] [0500/2226] mean_loss : 0.69\n",
      "[47/50] [1000/2226] mean_loss : 0.73\n",
      "[47/50] [1500/2226] mean_loss : 0.77\n",
      "[47/50] [2000/2226] mean_loss : 0.80\n",
      "[48/50] [0000/2226] mean_loss : 0.77\n",
      "[48/50] [0500/2226] mean_loss : 0.69\n",
      "[48/50] [1000/2226] mean_loss : 0.72\n",
      "[48/50] [1500/2226] mean_loss : 0.76\n",
      "[48/50] [2000/2226] mean_loss : 0.79\n",
      "[49/50] [0000/2226] mean_loss : 0.59\n",
      "[49/50] [0500/2226] mean_loss : 0.67\n",
      "[49/50] [1000/2226] mean_loss : 0.71\n",
      "[49/50] [1500/2226] mean_loss : 0.76\n",
      "[49/50] [2000/2226] mean_loss : 0.78\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    losses=[]\n",
    "    for i,batch in enumerate(getBatch(BATCH_SIZE,train_data)):\n",
    "        inputs,targets,input_lengths,target_lengths = pad_to_batch(batch,source2index,target2index)\n",
    "        \n",
    "        input_masks = torch.cat([Variable(ByteTensor(tuple(map(lambda s: s ==0, t.data))),volatile=False) for t in inputs]).view(inputs.size(0),-1)\n",
    "        start_decode = Variable(LongTensor([[target2index['<s>']]*targets.size(0)])).transpose(0,1)\n",
    "        #smaller_vocab = uniform_candidate_sampler(targets,200,len(word2index))\n",
    "        encoder.zero_grad()\n",
    "        decoder.zero_grad()\n",
    "        output, hidden_c = encoder(inputs,input_lengths)\n",
    "        \n",
    "        preds = decoder(start_decode,hidden_c,targets.size(1),output,input_masks,True)\n",
    "                                \n",
    "        loss = loss_function(preds,targets.view(-1))\n",
    "        losses.append(loss.data.tolist()[0] )\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(encoder.parameters(), 0.5) # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm(decoder.parameters(), 0.5) # gradient clipping\n",
    "        enc_optimizer.step()\n",
    "        dec_optimizer.step()\n",
    "\n",
    "    \n",
    "        if i % 500==0:\n",
    "            print(\"[%02d/%d] [%04d/%d] mean_loss : %0.2f\" %(epoch,EPOCH,i,len(train_data)//BATCH_SIZE,np.mean(losses)))\n",
    "            losses=[]\n",
    "#     torch.save(decoder.state_dict(),'../model/decoder.pkl')\n",
    "#     torch.save(encoder.state_dict(),'../model/', 'encoder.pkl')\n",
    "\n",
    "\n",
    "#     if (step+1) % 10 == 0:\n",
    "#         LR = LR/2\n",
    "#         enc_optimizer = optim.Adam(encoder.parameters(),lr=LR)\n",
    "#         dec_optimizer = optim.Adam(decoder.parameters(),lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# borrowed code from https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation-batched.ipynb\n",
    "\n",
    "def show_attention(input_words, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_words, rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "#     show_plot_visdom()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source :  i feel bad that i haven t paid you yet .\n",
      "Truth :  je me sens mal de ne pas encore vous avoir payee .\n",
      "Prediction :  je me sens mal de ne pas vous encore encore payees .\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUUAAAETCAYAAAChoxS3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIioSFCMiKhIUIyIqEhQjIir+PwpcyX7q0e7AAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f04e51fa898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = random.choice(train_data)\n",
    "input_ = test[0]\n",
    "truth = test[1]\n",
    "start_decode = Variable(LongTensor([[target2index['<s>']]*1])).transpose(0,1)\n",
    "\n",
    "output, hidden = encoder(input_,[input_.size(1)])\n",
    "pred,attn = decoder.decode(hidden,output)\n",
    "\n",
    "input_ = [index2source[i] for i in input_.data.tolist()[0]]\n",
    "pred = [index2target[i] for i in pred.data.tolist()]\n",
    "\n",
    "\n",
    "print('Source : ',' '.join([i for i in input_ if i not in ['<s>','</s>']]))\n",
    "print('Truth : ',' '.join([index2target[i] for i in truth.data.tolist()[0] if i not in [2,3]]))\n",
    "print('Prediction : ',' '.join([i for i in pred if i not in ['</s>']]))\n",
    "\n",
    "\n",
    "show_attention(input_,pred,attn.data.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* BLEU\n",
    "* Beam Search\n",
    "* Sampled Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
