{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Neural Machine Translation and Models with Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture9.pdf\n",
    "* http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture10.pdf\n",
    "* http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture11.pdf\n",
    "* https://arxiv.org/pdf/1409.0473.pdf\n",
    "* http://www.aclweb.org/anthology/P15-1001\n",
    "* https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation-batched.ipynb\n",
    "* http://www.manythings.org/anki/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter, OrderedDict\n",
    "import nltk\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "from torch.nn.utils.rnn import PackedSequence,pack_padded_sequence\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if USE_CUDA else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if USE_CUDA else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if USE_CUDA else torch.ByteTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getBatch(batch_size,train_data):\n",
    "    random.shuffle(train_data)\n",
    "    sindex=0\n",
    "    eindex=batch_size\n",
    "    while eindex < len(train_data):\n",
    "        batch = train_data[sindex:eindex]\n",
    "        temp = eindex\n",
    "        eindex = eindex+batch_size\n",
    "        sindex = temp\n",
    "        yield batch\n",
    "    \n",
    "    if eindex >= len(train_data):\n",
    "        batch = train_data[sindex:]\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_to_batch(batch,x_to_ix,y_to_ix):\n",
    "    \n",
    "    sorted_batch =  sorted(batch, key=lambda b:b[0].size(1),reverse=True) # sort by len\n",
    "    x,y = list(zip(*sorted_batch))\n",
    "    max_x = max([s.size(1) for s in x])\n",
    "    max_y = max([s.size(1) for s in y])\n",
    "    x_p,y_p=[],[]\n",
    "    for i in range(len(batch)):\n",
    "        if x[i].size(1)<max_x:\n",
    "            x_p.append(torch.cat([x[i],Variable(LongTensor([x_to_ix['<PAD>']]*(max_x-x[i].size(1)))).view(1,-1)],1))\n",
    "        else:\n",
    "            x_p.append(x[i])\n",
    "        if y[i].size(1)<max_y:\n",
    "            y_p.append(torch.cat([y[i],Variable(LongTensor([y_to_ix['<PAD>']]*(max_y-y[i].size(1)))).view(1,-1)],1))\n",
    "        else:\n",
    "            y_p.append(y[i])\n",
    "        \n",
    "    input_var = torch.cat(x_p)\n",
    "    target_var = torch.cat(y_p)\n",
    "    input_len = [list(map(lambda s: s ==0, t.data)).count(False) for t in input_var]\n",
    "    target_len = [list(map(lambda s: s ==0, t.data)).count(False) for t in target_var]\n",
    "    \n",
    "    return input_var, target_var, input_len, target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_index):\n",
    "    idxs = list(map(lambda w: to_index[w] if w in to_index.keys() else to_index[\"<UNK>\"], seq))\n",
    "    return Variable(LongTensor(idxs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data load and Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Borrowed code from https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation-batched.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalize_string(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([,.!?])\", r\" \\1 \", s)\n",
    "    s = re.sub(r\"[^a-zA-Z,.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = open('../dataset/eng-fra.txt','r',encoding='utf-8').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142787"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = corpus[:30000] # for practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MIN_LENGTH=3\n",
    "MAX_LENGTH=25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29830 29830\n",
      "['i', 'see', '.'] ['je', 'comprends', '.']\n",
      "CPU times: user 808 ms, sys: 12 ms, total: 820 ms\n",
      "Wall time: 820 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_r,y_r=[],[] # raw\n",
    "\n",
    "for parallel in corpus:\n",
    "    so,ta = parallel[:-1].split('\\t')\n",
    "    if so.strip()==\"\" or ta.strip()==\"\": continue\n",
    "    \n",
    "    normalized_so = normalize_string(so).split()\n",
    "    normalized_ta = normalize_string(ta).split()\n",
    "    \n",
    "    if len(normalized_so)>=MIN_LENGTH and len(normalized_so)<=MAX_LENGTH \\\n",
    "    and len(normalized_ta)>=MIN_LENGTH and len(normalized_ta)<=MAX_LENGTH:\n",
    "        X_r.append(normalized_so)\n",
    "        y_r.append(normalized_ta)\n",
    "    \n",
    "\n",
    "print(len(X_r),len(y_r))\n",
    "print(X_r[0],y_r[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4433 7704\n"
     ]
    }
   ],
   "source": [
    "source_vocab = list(set(flatten(X_r)))\n",
    "target_vocab = list(set(flatten(y_r)))\n",
    "print(len(source_vocab),len(target_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source2index = {'<PAD>':0,'<UNK>':1,'<s>':2,'</s>':3}\n",
    "for vo in source_vocab:\n",
    "    if vo not in source2index.keys():\n",
    "        source2index[vo]=len(source2index)\n",
    "index2source = {v:k for k,v in source2index.items()}\n",
    "\n",
    "target2index = {'<PAD>':0,'<UNK>':1,'<s>':2,'</s>':3}\n",
    "for vo in target_vocab:\n",
    "    if vo not in target2index.keys():\n",
    "        target2index[vo]=len(target2index)\n",
    "index2target = {v:k for k,v in target2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.12 s, sys: 216 ms, total: 2.34 s\n",
      "Wall time: 2.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_p,y_p=[],[]\n",
    "\n",
    "for so,ta in zip(X_r,y_r):\n",
    "    X_p.append(prepare_sequence(so+['</s>'],source2index).view(1,-1))\n",
    "    y_p.append(prepare_sequence(ta+['</s>'],target2index).view(1,-1))\n",
    "    \n",
    "train_data = list(zip(X_p,y_p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size,hidden_size, n_layers=1,bidirec=False):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        \n",
    "        if bidirec:\n",
    "            self.n_direction = 2 \n",
    "            self.gru = nn.GRU(embedding_size, hidden_size, n_layers, batch_first=True,bidirectional=True)\n",
    "        else:\n",
    "            self.n_direction = 1\n",
    "            self.gru = nn.GRU(embedding_size, hidden_size, n_layers, batch_first=True)\n",
    "    \n",
    "    def init_hidden(self,inputs):\n",
    "        hidden = Variable(torch.zeros(self.n_layers*self.n_direction,inputs.size(0),self.hidden_size))\n",
    "        return hidden.cuda() if USE_CUDA else hidden\n",
    "    \n",
    "    def init_weight(self):\n",
    "        self.embedding.weight = nn.init.xavier_uniform(self.embedding.weight)\n",
    "        self.gru.weight_hh_l0 = nn.init.xavier_uniform(self.gru.weight_hh_l0)\n",
    "        self.gru.weight_ih_l0 = nn.init.xavier_uniform(self.gru.weight_ih_l0)\n",
    "    \n",
    "    def forward(self, inputs, input_lengths):\n",
    "        \"\"\"\n",
    "        inputs : B,T (LongTensor)\n",
    "        input_lengths : real lengths of input batch (list)\n",
    "        \"\"\"\n",
    "        hidden = self.init_hidden(inputs)\n",
    "        \n",
    "        embedded = self.embedding(inputs)\n",
    "        packed = pack_padded_sequence(embedded, input_lengths,batch_first=True)\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs,batch_first=True) # unpack (back to padded)\n",
    "                \n",
    "        if self.n_layers>1:\n",
    "            if self.n_direction==2:\n",
    "                hidden = hidden[-2:]\n",
    "            else:\n",
    "                hidden = hidden[-1]\n",
    "        \n",
    "        return outputs, torch.cat(hidden,1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, n_layers=1,dropout_p=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # Define the layers\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        self.gru = nn.GRU(embedding_size+hidden_size, hidden_size, n_layers,batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size*2, input_size)\n",
    "        self.attn = nn.Linear(self.hidden_size,self.hidden_size) # Attention\n",
    "    \n",
    "    def init_hidden(self,inputs):\n",
    "        hidden = Variable(torch.zeros(self.n_layers,inputs.size(0),self.hidden_size))\n",
    "        return hidden.cuda() if USE_CUDA else hidden\n",
    "    \n",
    "    \n",
    "    def init_weight(self):\n",
    "        self.embedding.weight = nn.init.xavier_uniform(self.embedding.weight)\n",
    "        self.gru.weight_hh_l0 = nn.init.xavier_uniform(self.gru.weight_hh_l0)\n",
    "        self.gru.weight_ih_l0 = nn.init.xavier_uniform(self.gru.weight_ih_l0)\n",
    "        self.linear.weight = nn.init.xavier_uniform(self.linear.weight)\n",
    "        self.attn.weight = nn.init.xavier_uniform(self.attn.weight)\n",
    "        self.attn.bias.data.fill_(0)\n",
    "    \n",
    "    def Attention(self, hidden, encoder_outputs, encoder_maskings):\n",
    "        \"\"\"\n",
    "        hidden : 1,B,D\n",
    "        encoder_outputs : B,T,D\n",
    "        encoder_maskings : B,T # ByteTensor\n",
    "        \"\"\"\n",
    "        hidden = hidden[0].unsqueeze(2)  # (1,B,D) -> (B,D,1)\n",
    "        \n",
    "        batch_size = encoder_outputs.size(0) # B\n",
    "        max_len = encoder_outputs.size(1) # T\n",
    "        energies = self.attn(encoder_outputs.contiguous().view(batch_size*max_len,-1)) # B*T,D -> B*T,D\n",
    "        energies = energies.view(batch_size,max_len,-1) # B,T,D\n",
    "        attn_energies = energies.bmm(hidden).squeeze(2) # B,T,D * B,D,1 --> B,T\n",
    "        \n",
    "#         if isinstance(encoder_maskings,torch.autograd.variable.Variable):\n",
    "#             attn_energies = attn_energies.masked_fill(encoder_maskings,float('-inf'))#-1e12) # PAD masking\n",
    "        \n",
    "        alpha = F.softmax(attn_energies) # B,T\n",
    "        alpha = alpha.unsqueeze(1) # B,1,T\n",
    "        context = alpha.bmm(encoder_outputs) # B,1,T * B,T,D => B,1,D\n",
    "        \n",
    "        return context, alpha\n",
    "    \n",
    "    \n",
    "    def forward(self,inputs,context,max_length,encoder_outputs,encoder_maskings=None,is_training=False):\n",
    "        \"\"\"\n",
    "        inputs : B,1 (LongTensor, START SYMBOL)\n",
    "        context : B,1,D (FloatTensor, Last encoder hidden state)\n",
    "        max_length : int, max length to decode # for batch\n",
    "        encoder_outputs : B,T,D\n",
    "        encoder_maskings : B,T # ByteTensor\n",
    "        is_training : bool, this is because adapt dropout only training step.\n",
    "        \"\"\"\n",
    "        # Get the embedding of the current input word\n",
    "        embedded = self.embedding(inputs)\n",
    "        hidden = self.init_hidden(inputs)\n",
    "        if is_training:\n",
    "            embedded = self.dropout(embedded)\n",
    "        \n",
    "        decode=[]\n",
    "        # Apply GRU to the output so far\n",
    "        for i in range(max_length):\n",
    "\n",
    "            _, hidden = self.gru(torch.cat((embedded,context),2), hidden) # h_t = f(h_{t-1},y_{t-1},c)\n",
    "            concated = torch.cat((hidden,context.transpose(0,1)),2) # y_t = g(h_t,y_{t-1},c)\n",
    "            score = self.linear(concated.squeeze(0))\n",
    "            softmaxed = F.log_softmax(score)\n",
    "            decode.append(softmaxed)\n",
    "            decoded = softmaxed.max(1)[1]\n",
    "            embedded = self.embedding(decoded).unsqueeze(1) # y_{t-1}\n",
    "            if is_training:\n",
    "                embedded = self.dropout(embedded)\n",
    "            \n",
    "            # compute next context vector using attention\n",
    "            context, alpha = self.Attention(hidden, encoder_outputs,encoder_maskings)\n",
    "            \n",
    "        #  column-wise concat, reshape!!\n",
    "        scores = torch.cat(decode,1)\n",
    "        return scores.view(inputs.size(0)*max_length,-1)\n",
    "    \n",
    "    def decode(self,context,encoder_outputs):\n",
    "        start_decode = Variable(LongTensor([[target2index['<s>']]*1])).transpose(0,1)\n",
    "        embedded = self.embedding(start_decode)\n",
    "        hidden = self.init_hidden(start_decode)\n",
    "        \n",
    "        decodes=[]\n",
    "        attentions=[]\n",
    "        decoded = embedded\n",
    "        while decoded.data.tolist()[0]!=target2index['</s>']: # until </s>\n",
    "            _, hidden = self.gru(torch.cat((embedded,context),2), hidden) # h_t = f(h_{t-1},y_{t-1},c)\n",
    "            concated = torch.cat((hidden,context.transpose(0,1)),2) # y_t = g(h_t,y_{t-1},c)\n",
    "            score = self.linear(concated.squeeze(0))\n",
    "            softmaxed = F.log_softmax(score)\n",
    "            decodes.append(softmaxed)\n",
    "            decoded = softmaxed.max(1)[1]\n",
    "            embedded = self.embedding(decoded).unsqueeze(1) # y_{t-1}\n",
    "            context, alpha = self.Attention(hidden, encoder_outputs,None)\n",
    "            attentions.append(alpha.squeeze(1))\n",
    "        \n",
    "        return torch.cat(decodes).max(1)[1], torch.cat(attentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes for a while..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "EPOCH=50\n",
    "BATCH_SIZE = 64\n",
    "EMBEDDING_SIZE = 100\n",
    "HIDDEN_SIZE = 256\n",
    "LR = 0.0001\n",
    "DECODER_LEARNING_RATIO=5.0\n",
    "RESCHEDULED=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(len(source2index),EMBEDDING_SIZE,HIDDEN_SIZE,3,True)\n",
    "decoder = Decoder(len(target2index),EMBEDDING_SIZE,HIDDEN_SIZE*2)\n",
    "encoder.init_weight()\n",
    "decoder.init_weight()\n",
    "\n",
    "if USE_CUDA:\n",
    "    encoder = encoder.cuda()\n",
    "    decoder = decoder.cuda()\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
    "enc_optimizer = optim.Adam(encoder.parameters(),lr=LR)\n",
    "dec_optimizer = optim.Adam(decoder.parameters(),lr=LR*DECODER_LEARNING_RATIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enc_optimizer = optim.Adam(encoder.parameters(),lr=0.00001)\n",
    "dec_optimizer = optim.Adam(decoder.parameters(),lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/50] [000/466] mean_loss : 8.96\n",
      "[00/50] [200/466] mean_loss : 5.07\n",
      "[00/50] [400/466] mean_loss : 4.46\n",
      "[01/50] [000/466] mean_loss : 4.15\n",
      "[01/50] [200/466] mean_loss : 4.05\n",
      "[01/50] [400/466] mean_loss : 3.85\n",
      "[02/50] [000/466] mean_loss : 3.55\n",
      "[02/50] [200/466] mean_loss : 3.55\n",
      "[02/50] [400/466] mean_loss : 3.46\n",
      "[03/50] [000/466] mean_loss : 3.07\n",
      "[03/50] [200/466] mean_loss : 3.21\n",
      "[03/50] [400/466] mean_loss : 3.18\n",
      "[04/50] [000/466] mean_loss : 2.77\n",
      "[04/50] [200/466] mean_loss : 2.97\n",
      "[04/50] [400/466] mean_loss : 2.95\n",
      "[05/50] [000/466] mean_loss : 2.85\n",
      "[05/50] [200/466] mean_loss : 2.76\n",
      "[05/50] [400/466] mean_loss : 2.75\n",
      "[06/50] [000/466] mean_loss : 2.51\n",
      "[06/50] [200/466] mean_loss : 2.56\n",
      "[06/50] [400/466] mean_loss : 2.59\n",
      "[07/50] [000/466] mean_loss : 2.34\n",
      "[07/50] [200/466] mean_loss : 2.41\n",
      "[07/50] [400/466] mean_loss : 2.42\n",
      "[08/50] [000/466] mean_loss : 2.30\n",
      "[08/50] [200/466] mean_loss : 2.25\n",
      "[08/50] [400/466] mean_loss : 2.30\n",
      "[09/50] [000/466] mean_loss : 2.44\n",
      "[09/50] [200/466] mean_loss : 2.11\n",
      "[09/50] [400/466] mean_loss : 2.17\n",
      "[10/50] [000/466] mean_loss : 2.08\n",
      "[10/50] [200/466] mean_loss : 2.00\n",
      "[10/50] [400/466] mean_loss : 2.03\n",
      "[11/50] [000/466] mean_loss : 1.89\n",
      "[11/50] [200/466] mean_loss : 1.88\n",
      "[11/50] [400/466] mean_loss : 1.92\n",
      "[12/50] [000/466] mean_loss : 1.70\n",
      "[12/50] [200/466] mean_loss : 1.78\n",
      "[12/50] [400/466] mean_loss : 1.83\n",
      "[13/50] [000/466] mean_loss : 1.69\n",
      "[13/50] [200/466] mean_loss : 1.67\n",
      "[13/50] [400/466] mean_loss : 1.73\n",
      "[14/50] [000/466] mean_loss : 1.51\n",
      "[14/50] [200/466] mean_loss : 1.58\n",
      "[14/50] [400/466] mean_loss : 1.65\n",
      "[15/50] [000/466] mean_loss : 1.46\n",
      "[15/50] [200/466] mean_loss : 1.50\n",
      "[15/50] [400/466] mean_loss : 1.56\n",
      "[16/50] [000/466] mean_loss : 1.23\n",
      "[16/50] [200/466] mean_loss : 1.41\n",
      "[16/50] [400/466] mean_loss : 1.48\n",
      "[17/50] [000/466] mean_loss : 1.15\n",
      "[17/50] [200/466] mean_loss : 1.36\n",
      "[17/50] [400/466] mean_loss : 1.41\n",
      "[18/50] [000/466] mean_loss : 1.15\n",
      "[18/50] [200/466] mean_loss : 1.30\n",
      "[18/50] [400/466] mean_loss : 1.36\n",
      "[19/50] [000/466] mean_loss : 1.12\n",
      "[19/50] [200/466] mean_loss : 1.22\n",
      "[19/50] [400/466] mean_loss : 1.30\n",
      "[20/50] [000/466] mean_loss : 1.22\n",
      "[20/50] [200/466] mean_loss : 1.17\n",
      "[20/50] [400/466] mean_loss : 1.23\n",
      "[21/50] [000/466] mean_loss : 0.94\n",
      "[21/50] [200/466] mean_loss : 1.11\n",
      "[21/50] [400/466] mean_loss : 1.17\n",
      "[22/50] [000/466] mean_loss : 1.20\n",
      "[22/50] [200/466] mean_loss : 1.07\n",
      "[22/50] [400/466] mean_loss : 1.13\n",
      "[23/50] [000/466] mean_loss : 0.91\n",
      "[23/50] [200/466] mean_loss : 1.03\n",
      "[23/50] [400/466] mean_loss : 1.10\n",
      "[24/50] [000/466] mean_loss : 0.78\n",
      "[24/50] [200/466] mean_loss : 0.98\n",
      "[24/50] [400/466] mean_loss : 1.07\n",
      "[25/50] [000/466] mean_loss : 0.94\n",
      "[25/50] [200/466] mean_loss : 0.95\n",
      "[25/50] [400/466] mean_loss : 1.00\n",
      "[26/50] [000/466] mean_loss : 0.86\n",
      "[26/50] [200/466] mean_loss : 0.81\n",
      "[26/50] [400/466] mean_loss : 0.81\n",
      "[27/50] [000/466] mean_loss : 0.70\n",
      "[27/50] [200/466] mean_loss : 0.76\n",
      "[27/50] [400/466] mean_loss : 0.77\n",
      "[28/50] [000/466] mean_loss : 0.87\n",
      "[28/50] [200/466] mean_loss : 0.74\n",
      "[28/50] [400/466] mean_loss : 0.75\n",
      "[29/50] [000/466] mean_loss : 0.80\n",
      "[29/50] [200/466] mean_loss : 0.72\n",
      "[29/50] [400/466] mean_loss : 0.74\n",
      "[30/50] [000/466] mean_loss : 0.69\n",
      "[30/50] [200/466] mean_loss : 0.72\n",
      "[30/50] [400/466] mean_loss : 0.73\n",
      "[31/50] [000/466] mean_loss : 0.90\n",
      "[31/50] [200/466] mean_loss : 0.71\n",
      "[31/50] [400/466] mean_loss : 0.72\n",
      "[32/50] [000/466] mean_loss : 0.71\n",
      "[32/50] [200/466] mean_loss : 0.69\n",
      "[32/50] [400/466] mean_loss : 0.71\n",
      "[33/50] [000/466] mean_loss : 0.71\n",
      "[33/50] [200/466] mean_loss : 0.69\n",
      "[33/50] [400/466] mean_loss : 0.71\n",
      "[34/50] [000/466] mean_loss : 0.63\n",
      "[34/50] [200/466] mean_loss : 0.68\n",
      "[34/50] [400/466] mean_loss : 0.71\n",
      "[35/50] [000/466] mean_loss : 0.70\n",
      "[35/50] [200/466] mean_loss : 0.68\n",
      "[35/50] [400/466] mean_loss : 0.69\n",
      "[36/50] [000/466] mean_loss : 0.68\n",
      "[36/50] [200/466] mean_loss : 0.68\n",
      "[36/50] [400/466] mean_loss : 0.68\n",
      "[37/50] [000/466] mean_loss : 0.69\n",
      "[37/50] [200/466] mean_loss : 0.66\n",
      "[37/50] [400/466] mean_loss : 0.69\n",
      "[38/50] [000/466] mean_loss : 0.60\n",
      "[38/50] [200/466] mean_loss : 0.66\n",
      "[38/50] [400/466] mean_loss : 0.68\n",
      "[39/50] [000/466] mean_loss : 0.68\n",
      "[39/50] [200/466] mean_loss : 0.65\n",
      "[39/50] [400/466] mean_loss : 0.67\n",
      "[40/50] [000/466] mean_loss : 0.70\n",
      "[40/50] [200/466] mean_loss : 0.64\n",
      "[40/50] [400/466] mean_loss : 0.67\n",
      "[41/50] [000/466] mean_loss : 0.61\n",
      "[41/50] [200/466] mean_loss : 0.65\n",
      "[41/50] [400/466] mean_loss : 0.66\n",
      "[42/50] [000/466] mean_loss : 0.60\n",
      "[42/50] [200/466] mean_loss : 0.65\n",
      "[42/50] [400/466] mean_loss : 0.65\n",
      "[43/50] [000/466] mean_loss : 0.52\n",
      "[43/50] [200/466] mean_loss : 0.63\n",
      "[43/50] [400/466] mean_loss : 0.65\n",
      "[44/50] [000/466] mean_loss : 0.66\n",
      "[44/50] [200/466] mean_loss : 0.64\n",
      "[44/50] [400/466] mean_loss : 0.64\n",
      "[45/50] [000/466] mean_loss : 0.60\n",
      "[45/50] [200/466] mean_loss : 0.63\n",
      "[45/50] [400/466] mean_loss : 0.64\n",
      "[46/50] [000/466] mean_loss : 0.57\n",
      "[46/50] [200/466] mean_loss : 0.63\n",
      "[46/50] [400/466] mean_loss : 0.64\n",
      "[47/50] [000/466] mean_loss : 0.71\n",
      "[47/50] [200/466] mean_loss : 0.63\n",
      "[47/50] [400/466] mean_loss : 0.63\n",
      "[48/50] [000/466] mean_loss : 0.52\n",
      "[48/50] [200/466] mean_loss : 0.62\n",
      "[48/50] [400/466] mean_loss : 0.62\n",
      "[49/50] [000/466] mean_loss : 0.55\n",
      "[49/50] [200/466] mean_loss : 0.61\n",
      "[49/50] [400/466] mean_loss : 0.63\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    losses=[]\n",
    "    for i,batch in enumerate(getBatch(BATCH_SIZE,train_data)):\n",
    "        inputs,targets,input_lengths,target_lengths = pad_to_batch(batch,source2index,target2index)\n",
    "        \n",
    "        input_masks = torch.cat([Variable(ByteTensor(tuple(map(lambda s: s ==0 or s==3, t.data)))) for t in inputs]).view(inputs.size(0),-1)\n",
    "        start_decode = Variable(LongTensor([[target2index['<s>']]*targets.size(0)])).transpose(0,1)\n",
    "        #smaller_vocab = uniform_candidate_sampler(targets,200,len(word2index))\n",
    "        encoder.zero_grad()\n",
    "        decoder.zero_grad()\n",
    "        output, hidden_c = encoder(inputs,input_lengths)\n",
    "        \n",
    "        preds = decoder(start_decode,hidden_c,targets.size(1),output,input_masks,True)\n",
    "                                \n",
    "        loss = loss_function(preds,targets.view(-1))\n",
    "        losses.append(loss.data.tolist()[0] )\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(encoder.parameters(), 50.0) # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm(decoder.parameters(), 50.0) # gradient clipping\n",
    "        enc_optimizer.step()\n",
    "        dec_optimizer.step()\n",
    "\n",
    "    \n",
    "        if i % 200==0:\n",
    "            print(\"[%02d/%d] [%03d/%d] mean_loss : %0.2f\" %(epoch,EPOCH,i,len(train_data)//BATCH_SIZE,np.mean(losses)))\n",
    "            losses=[]\n",
    "\n",
    "    # You can use http://pytorch.org/docs/master/optim.html#how-to-adjust-learning-rate\n",
    "    if RESCHEDULED==False and epoch  == EPOCH//2:\n",
    "        LR = LR*0.1\n",
    "        enc_optimizer = optim.Adam(encoder.parameters(),lr=LR)\n",
    "        dec_optimizer = optim.Adam(decoder.parameters(),lr=LR*DECODER_LEARNING_RATIO)\n",
    "        RESCHEDULED=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# borrowed code from https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation-batched.ipynb\n",
    "\n",
    "def show_attention(input_words, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_words, rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "#     show_plot_visdom()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source :  they were lucky .\n",
      "Truth :  elles ont eu de la chance .\n",
      "Prediction :  elles eurent de de la chance .\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPMAAAEPCAYAAACX0fLpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1ddfd6c7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = random.choice(train_data)\n",
    "input_ = test[0]\n",
    "truth = test[1]\n",
    "\n",
    "output, hidden = encoder(input_,[input_.size(1)])\n",
    "pred,attn = decoder.decode(hidden,output)\n",
    "\n",
    "input_ = [index2source[i] for i in input_.data.tolist()[0]]\n",
    "pred = [index2target[i] for i in pred.data.tolist()]\n",
    "\n",
    "\n",
    "print('Source : ',' '.join([i for i in input_ if i not in ['</s>']]))\n",
    "print('Truth : ',' '.join([index2target[i] for i in truth.data.tolist()[0] if i not in [2,3]]))\n",
    "print('Prediction : ',' '.join([i for i in pred if i not in ['</s>']]))\n",
    "\n",
    "if USE_CUDA:\n",
    "    attn = attn.cpu()\n",
    "\n",
    "show_attention(input_,pred,attn.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* BLEU\n",
    "* Beam Search\n",
    "* Sampled Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
