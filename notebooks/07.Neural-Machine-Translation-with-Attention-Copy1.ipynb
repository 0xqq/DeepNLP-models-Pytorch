{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Neural Machine Translation and Models with Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture9.pdf\n",
    "* http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture10.pdf\n",
    "* http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture11.pdf\n",
    "* https://arxiv.org/pdf/1409.0473.pdf\n",
    "* http://www.aclweb.org/anthology/P15-1001\n",
    "* https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation-batched.ipynb\n",
    "* http://www.manythings.org/anki/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter, OrderedDict\n",
    "import nltk\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "from torch.nn.utils.rnn import PackedSequence,pack_padded_sequence\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if USE_CUDA else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if USE_CUDA else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if USE_CUDA else torch.ByteTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getBatch(batch_size,train_data):\n",
    "    random.shuffle(train_data)\n",
    "    sindex=0\n",
    "    eindex=batch_size\n",
    "    while eindex < len(train_data):\n",
    "        batch = train_data[sindex:eindex]\n",
    "        temp = eindex\n",
    "        eindex = eindex+batch_size\n",
    "        sindex = temp\n",
    "        yield batch\n",
    "    \n",
    "    if eindex >= len(train_data):\n",
    "        batch = train_data[sindex:]\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_to_batch(batch,x_to_ix,y_to_ix):\n",
    "    \n",
    "    sorted_batch =  sorted(batch, key=lambda b:b[0].size(1),reverse=True) # sort by len\n",
    "    x,y = list(zip(*sorted_batch))\n",
    "    max_x = max([s.size(1) for s in x])\n",
    "    max_y = max([s.size(1) for s in y])\n",
    "    x_p,y_p=[],[]\n",
    "    for i in range(len(batch)):\n",
    "        if x[i].size(1)<max_x:\n",
    "            x_p.append(torch.cat([x[i],Variable(LongTensor([x_to_ix['<PAD>']]*(max_x-x[i].size(1)))).view(1,-1)],1))\n",
    "        else:\n",
    "            x_p.append(x[i])\n",
    "        if y[i].size(1)<max_y:\n",
    "            y_p.append(torch.cat([y[i],Variable(LongTensor([y_to_ix['<PAD>']]*(max_y-y[i].size(1)))).view(1,-1)],1))\n",
    "        else:\n",
    "            y_p.append(y[i])\n",
    "        \n",
    "    input_var = torch.cat(x_p)\n",
    "    target_var = torch.cat(y_p)\n",
    "    input_len = [list(map(lambda s: s ==0, t.data)).count(False) for t in input_var]\n",
    "    target_len = [list(map(lambda s: s ==0, t.data)).count(False) for t in target_var]\n",
    "    \n",
    "    return input_var, target_var, input_len, target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_index):\n",
    "    idxs = list(map(lambda w: to_index[w] if w in to_index.keys() else to_index[\"<UNK>\"], seq))\n",
    "    return Variable(LongTensor(idxs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data load and Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Borrowed code from https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation-batched.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalize_string(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([,.!?])\", r\" \\1 \", s)\n",
    "    s = re.sub(r\"[^a-zA-Z,.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = open('../dataset/eng-fra.txt','r',encoding='utf-8').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142787"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = corpus[:30000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MIN_LENGTH=3\n",
    "MAX_LENGTH=25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29830 29830\n",
      "['i', 'see', '.'] ['je', 'comprends', '.']\n",
      "CPU times: user 788 ms, sys: 4 ms, total: 792 ms\n",
      "Wall time: 791 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_r,y_r=[],[] # raw\n",
    "\n",
    "for parallel in corpus:\n",
    "    so,ta = parallel[:-1].split('\\t')\n",
    "    if so.strip()==\"\" or ta.strip()==\"\": continue\n",
    "    \n",
    "    normalized_so = normalize_string(so).split()\n",
    "    normalized_ta = normalize_string(ta).split()\n",
    "    \n",
    "    if len(normalized_so)>=MIN_LENGTH and len(normalized_so)<=MAX_LENGTH \\\n",
    "    and len(normalized_ta)>=MIN_LENGTH and len(normalized_ta)<=MAX_LENGTH:\n",
    "        X_r.append(normalized_so)\n",
    "        y_r.append(normalized_ta)\n",
    "    \n",
    "\n",
    "print(len(X_r),len(y_r))\n",
    "print(X_r[0],y_r[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4433 7704\n"
     ]
    }
   ],
   "source": [
    "source_vocab = list(set(flatten(X_r)))\n",
    "target_vocab = list(set(flatten(y_r)))\n",
    "print(len(source_vocab),len(target_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source2index = {'<PAD>':0,'<UNK>':1,'<s>':2,'</s>':3}\n",
    "for vo in source_vocab:\n",
    "    if vo not in source2index.keys():\n",
    "        source2index[vo]=len(source2index)\n",
    "index2source = {v:k for k,v in source2index.items()}\n",
    "\n",
    "target2index = {'<PAD>':0,'<UNK>':1,'<s>':2,'</s>':3}\n",
    "for vo in target_vocab:\n",
    "    if vo not in target2index.keys():\n",
    "        target2index[vo]=len(target2index)\n",
    "index2target = {v:k for k,v in target2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.19 s, sys: 232 ms, total: 2.42 s\n",
      "Wall time: 2.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_p,y_p=[],[]\n",
    "\n",
    "for so,ta in zip(X_r,y_r):\n",
    "    X_p.append(prepare_sequence(so+['</s>'],source2index).view(1,-1))\n",
    "    y_p.append(prepare_sequence(ta+['</s>'],target2index).view(1,-1))\n",
    "    \n",
    "train_data = list(zip(X_p,y_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = train_data[int(len(train_data)*0.7):]\n",
    "train_data = train_data[:int(len(train_data)*0.7)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size,hidden_size, n_layers=1,bidirec=False):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        \n",
    "        if bidirec:\n",
    "            self.n_direction = 2 \n",
    "            self.gru = nn.GRU(embedding_size, hidden_size, n_layers, batch_first=True,bidirectional=True)\n",
    "        else:\n",
    "            self.n_direction = 1\n",
    "            self.gru = nn.GRU(embedding_size, hidden_size, n_layers, batch_first=True)\n",
    "    \n",
    "    def init_hidden(self,inputs):\n",
    "        hidden = Variable(torch.zeros(self.n_layers*self.n_direction,inputs.size(0),self.hidden_size))\n",
    "        return hidden.cuda() if USE_CUDA else hidden\n",
    "    \n",
    "    def init_weight(self):\n",
    "        self.embedding.weight = nn.init.xavier_uniform(self.embedding.weight)\n",
    "#         self.gru.weight_hh_l0 = nn.init.xavier_uniform(self.gru.weight_hh_l0)\n",
    "#         self.gru.weight_ih_l0 = nn.init.xavier_uniform(self.gru.weight_ih_l0)\n",
    "    \n",
    "    def forward(self, inputs, input_lengths):\n",
    "        \"\"\"\n",
    "        inputs : B,T (LongTensor)\n",
    "        input_lengths : real lengths of input batch (list)\n",
    "        \"\"\"\n",
    "        hidden = self.init_hidden(inputs)\n",
    "        \n",
    "        embedded = self.embedding(inputs)\n",
    "        packed = pack_padded_sequence(embedded, input_lengths,batch_first=True)\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs,batch_first=True) # unpack (back to padded)\n",
    "        \n",
    "        if self.n_layers>1:\n",
    "            if self.n_direction==2:\n",
    "                hidden = hidden[-2:]\n",
    "            else:\n",
    "                hidden = hidden[-1]\n",
    "        \n",
    "        return outputs, torch.cat(hidden,1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, n_layers=1,dropout_p=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # Define the layers\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        self.gru = nn.GRU(embedding_size+hidden_size, hidden_size, n_layers,batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size*2, input_size)\n",
    "        self.attn = nn.Linear(self.hidden_size,self.hidden_size) # Attention\n",
    "        \n",
    "    def init_hidden(self,inputs):\n",
    "        hidden = Variable(torch.zeros(self.n_layers,inputs.size(0),self.hidden_size))\n",
    "        return hidden.cuda() if USE_CUDA else hidden\n",
    "    \n",
    "    \n",
    "    def init_weight(self):\n",
    "        self.embedding.weight = nn.init.xavier_uniform(self.embedding.weight)\n",
    "#         self.gru.weight_hh_l0 = nn.init.xavier_uniform(self.gru.weight_hh_l0)\n",
    "#         self.gru.weight_ih_l0 = nn.init.xavier_uniform(self.gru.weight_ih_l0)\n",
    "        self.linear.weight = nn.init.xavier_uniform(self.linear.weight)\n",
    "        self.attn.weight = nn.init.xavier_uniform(self.attn.weight)\n",
    "        self.attn.bias.data.fill_(0)\n",
    "    \n",
    "    def Attention(self, hidden, encoder_outputs, encoder_maskings):\n",
    "        \"\"\"\n",
    "        hidden : 1,B,D\n",
    "        encoder_outputs : B,T,D\n",
    "        encoder_maskings : B,T # ByteTensor\n",
    "        \"\"\"\n",
    "        hidden = hidden[0].unsqueeze(2)  # (1,B,D) -> (B,D,1)\n",
    "        \n",
    "        batch_size = encoder_outputs.size(0) # B\n",
    "        max_len = encoder_outputs.size(1) # T\n",
    "        energies = self.attn(encoder_outputs.contiguous().view(batch_size*max_len,-1)) # B*T,D -> B*T,D\n",
    "        energies = energies.view(batch_size,max_len,-1) # B,T,D\n",
    "        attn_energies = energies.bmm(hidden).squeeze(2) # B,T,D * B,D,1 --> B,T\n",
    "        \n",
    "        if isinstance(encoder_maskings,torch.autograd.variable.Variable):\n",
    "            attn_energies = attn_energies.masked_fill(encoder_maskings,-1e12) # PAD masking\n",
    "        \n",
    "        alpha = F.softmax(attn_energies) # B,T\n",
    "        alpha = alpha.unsqueeze(1) # B,1,T\n",
    "        context = alpha.bmm(encoder_outputs) # B,1,T * B,T,D => B,1,D\n",
    "        \n",
    "        return context, alpha\n",
    "    \n",
    "    \n",
    "    def forward(self,inputs,context,max_length,encoder_outputs,encoder_maskings=None,is_training=False):\n",
    "        \"\"\"\n",
    "        inputs : B,1 (LongTensor, START SYMBOL)\n",
    "        context : B,1,D (FloatTensor, Last encoder hidden state)\n",
    "        max_length : int, max length to decode # for batch\n",
    "        encoder_outputs : B,T,D\n",
    "        encoder_maskings : B,T # ByteTensor\n",
    "        is_training : bool, this is because adapt dropout only training step.\n",
    "        \"\"\"\n",
    "        # Get the embedding of the current input word\n",
    "        embedded = self.embedding(inputs)\n",
    "        hidden = self.init_hidden(inputs)\n",
    "        if is_training:\n",
    "            embedded = self.dropout(embedded)\n",
    "        \n",
    "        decode=[]\n",
    "        # Apply GRU to the output so far\n",
    "        for i in range(max_length):\n",
    "\n",
    "            _, hidden = self.gru(torch.cat((embedded,context),2), hidden) # h_t = f(h_{t-1},y_{t-1},c)\n",
    "            concated = torch.cat((hidden,context.transpose(0,1)),2) # y_t = g(h_t,y_{t-1},c)\n",
    "            score = self.linear(concated.squeeze(0))\n",
    "            softmaxed = F.log_softmax(score)\n",
    "            decode.append(softmaxed)\n",
    "            decoded = softmaxed.max(1)[1]\n",
    "            embedded = self.embedding(decoded).unsqueeze(1) # y_{t-1}\n",
    "            if is_training:\n",
    "                embedded = self.dropout(embedded)\n",
    "            \n",
    "            # compute next context vector using attention\n",
    "            context, alpha = self.Attention(hidden, encoder_outputs,encoder_maskings)\n",
    "        #  column-wise concat, reshape!!\n",
    "        scores = torch.cat(decode,1)\n",
    "        return scores.view(inputs.size(0)*max_length,-1)\n",
    "    \n",
    "    def decode(self,context,encoder_outputs):\n",
    "        start_decode = Variable(LongTensor([[target2index['<s>']]*1])).transpose(0,1)\n",
    "        embedded = self.embedding(start_decode)\n",
    "        hidden = self.init_hidden(start_decode)\n",
    "        \n",
    "        decodes=[]\n",
    "        attentions=[]\n",
    "        decoded = embedded\n",
    "        while decoded.data.tolist()[0]!=target2index['</s>']: # until </s>\n",
    "            _, hidden = self.gru(torch.cat((embedded,context),2), hidden) # h_t = f(h_{t-1},y_{t-1},c)\n",
    "            concated = torch.cat((hidden,context.transpose(0,1)),2) # y_t = g(h_t,y_{t-1},c)\n",
    "            score = self.linear(concated.squeeze(0))\n",
    "            softmaxed = F.log_softmax(score)\n",
    "            decodes.append(softmaxed)\n",
    "            decoded = softmaxed.max(1)[1]\n",
    "            embedded = self.embedding(decoded).unsqueeze(1) # y_{t-1}\n",
    "            context, alpha = self.Attention(hidden, encoder_outputs,None)\n",
    "            attentions.append(alpha.squeeze(1))\n",
    "        \n",
    "        return torch.cat(decodes).max(1)[1], torch.cat(attentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes for a while..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPOCH=50\n",
    "BATCH_SIZE = 64\n",
    "EMBEDDING_SIZE = 100\n",
    "HIDDEN_SIZE = 256\n",
    "LR = 0.0001\n",
    "DECODER_LEARNING_RATIO=5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(len(source2index),EMBEDDING_SIZE,HIDDEN_SIZE,3,True)\n",
    "decoder = Decoder(len(target2index),EMBEDDING_SIZE,HIDDEN_SIZE*2)\n",
    "encoder.init_weight()\n",
    "decoder.init_weight()\n",
    "\n",
    "if USE_CUDA:\n",
    "    encoder = encoder.cuda()\n",
    "    decoder = decoder.cuda()\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
    "enc_optimizer = optim.Adam(encoder.parameters(),lr=LR)\n",
    "dec_optimizer = optim.Adam(decoder.parameters(),lr=LR*DECODER_LEARNING_RATIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoder.load_state_dict(torch.load('../model/tencoder.pkl'))\n",
    "decoder.load_state_dict(torch.load('../model/tdecoder.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.save(decoder.state_dict(),'../model/tdecoder.pkl')\n",
    "torch.save(encoder.state_dict(),'../model/tencoder.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/50] [0000/326] mean_loss : 8.96\n",
      "[00/50] [0100/326] mean_loss : 5.34\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-af35b35fc05e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mlosses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_lengths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_to_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msource2index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget2index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0minput_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvolatile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-04515d662d80>\u001b[0m in \u001b[0;36mpad_to_batch\u001b[0;34m(batch, x_to_ix, y_to_ix)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mtarget_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0minput_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_var\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mtarget_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarget_var\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minput_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-04515d662d80>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mtarget_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0minput_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_var\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mtarget_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarget_var\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minput_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnelement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_range\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    losses=[]\n",
    "    for i,batch in enumerate(getBatch(BATCH_SIZE,train_data)):\n",
    "        inputs,targets,input_lengths,target_lengths = pad_to_batch(batch,source2index,target2index)\n",
    "        \n",
    "        input_masks = torch.cat([Variable(ByteTensor(tuple(map(lambda s: s ==0 or s==3, t.data))),volatile=False) for t in inputs]).view(inputs.size(0),-1)\n",
    "        start_decode = Variable(LongTensor([[target2index['<s>']]*targets.size(0)])).transpose(0,1)\n",
    "        #smaller_vocab = uniform_candidate_sampler(targets,200,len(word2index))\n",
    "        encoder.zero_grad()\n",
    "        decoder.zero_grad()\n",
    "        output, hidden_c = encoder(inputs,input_lengths)\n",
    "        \n",
    "        preds = decoder(start_decode,hidden_c,targets.size(1),output,input_masks,True)\n",
    "                                \n",
    "        loss = loss_function(preds,targets.view(-1))\n",
    "        losses.append(loss.data.tolist()[0] )\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(encoder.parameters(), 0.5) # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm(decoder.parameters(), 0.5) # gradient clipping\n",
    "        enc_optimizer.step()\n",
    "        dec_optimizer.step()\n",
    "\n",
    "    \n",
    "        if i % 100==0:\n",
    "            print(\"[%02d/%d] [%04d/%d] mean_loss : %0.2f\" %(epoch,EPOCH,i,len(train_data)//BATCH_SIZE,np.mean(losses)))\n",
    "            losses=[]\n",
    "#     torch.save(decoder.state_dict(),'../model/decoder.pkl')\n",
    "#     torch.save(encoder.state_dict(),'../model/', 'encoder.pkl')\n",
    "\n",
    "\n",
    "#     if (step+1) % 10 == 0:\n",
    "#         LR = LR/2\n",
    "#         enc_optimizer = optim.Adam(encoder.parameters(),lr=LR)\n",
    "#         dec_optimizer = optim.Adam(decoder.parameters(),lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# borrowed code from https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation-batched.ipynb\n",
    "\n",
    "def show_attention(input_words, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_words, rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "#     show_plot_visdom()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source :  i love movies .\n",
      "Truth :  j adore le cinema .\n",
      "Prediction :  je est a .\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAToAAAEYCAYAAADMJjphAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff5b027d668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = random.choice(train_data)\n",
    "input_ = test[0]\n",
    "truth = test[1]\n",
    "# start_decode = Variable(LongTensor([[target2index['<s>']]*1])).transpose(0,1)\n",
    "\n",
    "output, hidden = encoder(input_,[input_.size(1)])\n",
    "pred,attn = decoder.decode(hidden,output)\n",
    "\n",
    "input_ = [index2source[i] for i in input_.data.tolist()[0]]\n",
    "pred = [index2target[i] for i in pred.data.tolist()]\n",
    "\n",
    "\n",
    "print('Source : ',' '.join([i for i in input_ if i not in ['</s>']]))\n",
    "print('Truth : ',' '.join([index2target[i] for i in truth.data.tolist()[0] if i not in [2,3]]))\n",
    "print('Prediction : ',' '.join([i for i in pred if i not in ['</s>']]))\n",
    "\n",
    "if USE_CUDA:\n",
    "    attn = attn.cpu()\n",
    "\n",
    "show_attention(input_,pred,attn.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* BLEU\n",
    "* Beam Search\n",
    "* Sampled Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
