{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.Tree Recursive Neural Networks and Constituency Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture14-TreeRNNs.pdf\n",
    "* https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter, OrderedDict\n",
    "import nltk\n",
    "from copy import deepcopy\n",
    "import os\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if USE_CUDA else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if USE_CUDA else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if USE_CUDA else torch.ByteTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getBatch(batch_size,train_data):\n",
    "    random.shuffle(train_data)\n",
    "    sindex=0\n",
    "    eindex=batch_size\n",
    "    while eindex < len(train_data):\n",
    "        batch = train_data[sindex:eindex]\n",
    "        temp = eindex\n",
    "        eindex = eindex+batch_size\n",
    "        sindex = temp\n",
    "        yield batch\n",
    "    \n",
    "    if eindex >= len(train_data):\n",
    "        batch = train_data[sindex:]\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data load and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stanford Sentiment Treebank(https://nlp.stanford.edu/sentiment/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../dataset/trees/train.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-189e9a6e0507>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../dataset/trees/train.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../dataset/trees/train.txt'"
     ]
    }
   ],
   "source": [
    "sample = random.choice(open('../dataset/trees/train.txt','r',encoding='utf-8').readlines())\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tree import Tree as nltkTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Tree('0', [Tree('0', [Tree('3', ['Very']), Tree('0', [Tree('1', [Tree('0', ['stupid']), Tree('2', ['and'])]), Tree('0', ['annoying'])])]), Tree('2', ['.'])])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltkTree.fromstring(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree Class "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "borrowed code from https://github.com/bogatyy/cs224d/tree/master/assignment3 (thanks!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Node:  # a node in the tree\n",
    "    def __init__(self, label, word=None):\n",
    "        self.label = label\n",
    "        self.word = word\n",
    "        self.parent = None  # reference to parent\n",
    "        self.left = None  # reference to left child\n",
    "        self.right = None  # reference to right child\n",
    "        # true if I am a leaf (could have probably derived this from if I have\n",
    "        # a word)\n",
    "        self.isLeaf = False\n",
    "        # true if we have finished performing fowardprop on this node (note,\n",
    "        # there are many ways to implement the recursion.. some might not\n",
    "        # require this flag)\n",
    "\n",
    "    def __str__(self):\n",
    "        if self.isLeaf:\n",
    "            return '[{0}:{1}]'.format(self.word, self.label)\n",
    "        return '({0} <- [{1}:{2}] -> {3})'.format(self.left, self.word, self.label, self.right)\n",
    "\n",
    "\n",
    "class Tree:\n",
    "\n",
    "    def __init__(self, treeString, openChar='(', closeChar=')'):\n",
    "        tokens = []\n",
    "        self.open = '('\n",
    "        self.close = ')'\n",
    "        for toks in treeString.strip().split():\n",
    "            tokens += list(toks)\n",
    "        self.root = self.parse(tokens)\n",
    "        # get list of labels as obtained through a post-order traversal\n",
    "        self.labels = get_labels(self.root)\n",
    "        self.num_words = len(self.labels)\n",
    "\n",
    "    def parse(self, tokens, parent=None):\n",
    "        assert tokens[0] == self.open, \"Malformed tree\"\n",
    "        assert tokens[-1] == self.close, \"Malformed tree\"\n",
    "\n",
    "        split = 2  # position after open and label\n",
    "        countOpen = countClose = 0\n",
    "\n",
    "        if tokens[split] == self.open:\n",
    "            countOpen += 1\n",
    "            split += 1\n",
    "        # Find where left child and right child split\n",
    "        while countOpen != countClose:\n",
    "            if tokens[split] == self.open:\n",
    "                countOpen += 1\n",
    "            if tokens[split] == self.close:\n",
    "                countClose += 1\n",
    "            split += 1\n",
    "\n",
    "        # New node\n",
    "        node = Node(int(tokens[1]))  # zero index labels\n",
    "\n",
    "        node.parent = parent\n",
    "\n",
    "        # leaf Node\n",
    "        if countOpen == 0:\n",
    "            node.word = ''.join(tokens[2:-1]).lower()  # lower case?\n",
    "            node.isLeaf = True\n",
    "            return node\n",
    "\n",
    "        node.left = self.parse(tokens[2:split], parent=node)\n",
    "        node.right = self.parse(tokens[split:-1], parent=node)\n",
    "\n",
    "        return node\n",
    "\n",
    "    def get_words(self):\n",
    "        leaves = getLeaves(self.root)\n",
    "        words = [node.word for node in leaves]\n",
    "        return words\n",
    "\n",
    "def get_labels(node):\n",
    "    if node is None:\n",
    "        return []\n",
    "    return get_labels(node.left) + get_labels(node.right) + [node.label]\n",
    "\n",
    "def getLeaves(node):\n",
    "    if node is None:\n",
    "        return []\n",
    "    if node.isLeaf:\n",
    "        return [node]\n",
    "    else:\n",
    "        return getLeaves(node.left) + getLeaves(node.right)\n",
    "\n",
    "    \n",
    "def loadTrees(dataSet='train'):\n",
    "    \"\"\"\n",
    "    Loads training trees. Maps leaf node words to word ids.\n",
    "    \"\"\"\n",
    "    file = '../dataset/trees/%s.txt' % dataSet\n",
    "    print(\"Loading %s trees..\" % dataSet)\n",
    "    with open(file, 'r',encoding='utf-8') as fid:\n",
    "        trees = [Tree(l) for l in fid.readlines()]\n",
    "\n",
    "    return trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train trees..\n"
     ]
    }
   ],
   "source": [
    "train_data = loadTrees('train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Vocab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = list(set(flatten([t.get_words() for t in train_data])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2index={'<UNK>':0}\n",
    "for vo in vocab:\n",
    "    if vo not in word2index.keys():\n",
    "        word2index[vo]=len(word2index)\n",
    "        \n",
    "index2word = {v:k for k,v in word2index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RNTN(nn.Module):\n",
    "    \n",
    "    def __init__(self,word2index,hidden_size,output_size):\n",
    "        super(RNTN,self).__init__()\n",
    "        \n",
    "        self.word2index = word2index\n",
    "        self.embed = nn.Embedding(len(word2index),hidden_size)\n",
    "#         self.V = nn.ModuleList([nn.Linear(hidden_size*2,hidden_size*2) for _ in range(hidden_size)])\n",
    "#         self.W = nn.Linear(hidden_size*2,hidden_size)\n",
    "        self.V = nn.ParameterList([nn.Parameter(torch.randn(hidden_size*2,hidden_size*2)) for _ in range(hidden_size)]) # Tensor\n",
    "        self.W = nn.Parameter(torch.randn(hidden_size*2,hidden_size))\n",
    "        self.b = nn.Parameter(torch.randn(1,hidden_size))\n",
    "#         self.W_out = nn.Parameter(torch.randn(hidden_size,output_size))\n",
    "        self.W_out = nn.Linear(hidden_size,output_size)\n",
    "        \n",
    "    def init_weight(self):\n",
    "        nn.init.xavier_uniform(self.embed.state_dict()['weight'])\n",
    "        nn.init.xavier_uniform(self.W_out.state_dict()['weight'])\n",
    "        for param in self.V.parameters():\n",
    "            nn.init.xavier_uniform(param)\n",
    "        nn.init.xavier_uniform(self.W)\n",
    "        self.b.data.fill_(0)\n",
    "#         nn.init.xavier_uniform(self.W_out)\n",
    "        \n",
    "    def tree_propagation(self,node):\n",
    "        \n",
    "        recursive_tensor = OrderedDict()\n",
    "        current=None\n",
    "        if node.isLeaf:\n",
    "            tensor = Variable(LongTensor([self.word2index[node.word]])) if node.word in self.word2index.keys() \\\n",
    "                          else Variable(LongTensor([self.word2index['<UNK>']]))\n",
    "            current = self.embed(tensor) # 1xD\n",
    "        else:\n",
    "            recursive_tensor.update(self.tree_propagation(node.left))\n",
    "            recursive_tensor.update(self.tree_propagation(node.right))\n",
    "            \n",
    "            concated = torch.cat([recursive_tensor[node.left],recursive_tensor[node.right]],1) # 1x2D\n",
    "            xVx=[] \n",
    "            for i,v in enumerate(self.V):\n",
    "#                 xVx.append(torch.matmul(v(concated),concated.transpose(0,1)))\n",
    "                xVx.append(torch.matmul(torch.matmul(concated,v),concated.transpose(0,1)))\n",
    "            \n",
    "            xVx = torch.cat(xVx,1) # 1xD\n",
    "#             Wx = self.W(concated)\n",
    "            Wx = torch.matmul(concated,self.W) # 1xD\n",
    "\n",
    "            current = F.tanh(xVx+Wx+self.b) # 1xD\n",
    "        recursive_tensor[node]=current\n",
    "        return recursive_tensor\n",
    "        \n",
    "    def forward(self,Trees,root_only=False):\n",
    "        \n",
    "        propagated=[]\n",
    "        if not isinstance(Trees,list):\n",
    "            Trees = [Trees]\n",
    "            \n",
    "        for Tree in Trees:\n",
    "            recursive_tensor = self.tree_propagation(Tree.root)\n",
    "            if root_only:\n",
    "                recursive_tensor = recursive_tensor[Tree.root]\n",
    "                propagated.append(recursive_tensor)\n",
    "            else:\n",
    "                recursive_tensor = [tensor for node,tensor in recursive_tensor.items()]\n",
    "                propagated.extend(recursive_tensor)\n",
    "        \n",
    "        propagated = torch.cat(propagated) # (num_of_node in batch, D)\n",
    "        \n",
    "#         return F.log_softmax(propagated.matmul(self.W_out))\n",
    "        return F.log_softmax(self.W_out(propagated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes for a while... It makes computational graph dynamically. So Its computation is difficult to train with batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 30\n",
    "ROOT_ONLY = False\n",
    "BATCH_SIZE = 20\n",
    "EPOCH = 20\n",
    "LR = 0.01\n",
    "LAMBDA = 1e-5\n",
    "RESCHEDULED=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = RNTN(word2index,HIDDEN_SIZE,5)\n",
    "model.init_weight()\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/20] mean_loss : 1.59\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    losses=[]\n",
    "    \n",
    "    # learning rate annealing\n",
    "    if RESCHEDULED==False and epoch==EPOCH//2:\n",
    "        LR=LR*0.1\n",
    "        optimizer = optim.Adam(model.parameters(),lr=LR,weight_decay=LAMBDA) # L2 norm\n",
    "        RESCHEDULED=True\n",
    "    \n",
    "    for i, batch in enumerate(getBatch(BATCH_SIZE,train_data)):\n",
    "        \n",
    "        if ROOT_ONLY:\n",
    "            labels = [tree.labels[-1] for tree in batch]\n",
    "            labels = Variable(LongTensor(labels))\n",
    "        else:\n",
    "            labels = [tree.labels for tree in batch]\n",
    "            labels = Variable(LongTensor(flatten(labels)))\n",
    "        \n",
    "        model.zero_grad()\n",
    "        preds = model(batch,ROOT_ONLY)\n",
    "        \n",
    "        loss = loss_function(preds,labels)\n",
    "        losses.append(loss.data.tolist()[0])\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 100==0:\n",
    "            print('[%d/%d] mean_loss : %.2f' % (epoch,EPOCH,np.mean(losses)))\n",
    "            losses=[]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test trees..\n"
     ]
    }
   ],
   "source": [
    "test_data = loadTrees('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy=0\n",
    "num_node=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-grained all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In paper, they acheived 80.2 accuracy. In my case, I don't use L2-norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78.84940140999805\n"
     ]
    }
   ],
   "source": [
    "for test in test_data:\n",
    "    model.zero_grad()\n",
    "    preds = model(test,ROOT_ONLY)\n",
    "    labels = test.labels[-1:] if ROOT_ONLY else test.labels\n",
    "    for pred,label in zip(preds.max(1)[1].data.tolist(),labels):\n",
    "        num_node+=1\n",
    "        if pred==label:\n",
    "            accuracy+=1\n",
    "\n",
    "print(accuracy/num_node*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://github.com/nearai/pytorch-tools # Dynamic batch using TensorFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
