{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. \tDynamic Memory Networks for Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture16-DMN-QA.pdf\n",
    "* https://arxiv.org/abs/1506.07285"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter, OrderedDict\n",
    "import nltk\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "from torch.nn.utils.rnn import PackedSequence,pack_padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if USE_CUDA else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if USE_CUDA else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if USE_CUDA else torch.ByteTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getBatch(batch_size,train_data):\n",
    "    random.shuffle(train_data)\n",
    "    sindex=0\n",
    "    eindex=batch_size\n",
    "    while eindex < len(train_data):\n",
    "        batch = train_data[sindex:eindex]\n",
    "        temp = eindex\n",
    "        eindex = eindex+batch_size\n",
    "        sindex = temp\n",
    "        yield batch\n",
    "    \n",
    "    if eindex >= len(train_data):\n",
    "        batch = train_data[sindex:]\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_to_batch(batch,x_to_ix,y_to_ix):\n",
    "    \n",
    "    sorted_batch =  sorted(batch, key=lambda b:b[0].size(1),reverse=True) # sort by len\n",
    "    x,y = list(zip(*sorted_batch))\n",
    "    max_x = max([s.size(1) for s in x])\n",
    "    max_y = max([s.size(1) for s in y])\n",
    "    x_p,y_p=[],[]\n",
    "    for i in range(len(batch)):\n",
    "        if x[i].size(1)<max_x:\n",
    "            x_p.append(torch.cat([x[i],Variable(LongTensor([x_to_ix['<PAD>']]*(max_x-x[i].size(1)))).view(1,-1)],1))\n",
    "        else:\n",
    "            x_p.append(x[i])\n",
    "        if y[i].size(1)<max_y:\n",
    "            y_p.append(torch.cat([y[i],Variable(LongTensor([y_to_ix['<PAD>']]*(max_y-y[i].size(1)))).view(1,-1)],1))\n",
    "        else:\n",
    "            y_p.append(y[i])\n",
    "        \n",
    "    input_var = torch.cat(x_p)\n",
    "    target_var = torch.cat(y_p)\n",
    "    input_len = [list(map(lambda s: s ==0, t.data)).count(False) for t in input_var]\n",
    "    target_len = [list(map(lambda s: s ==0, t.data)).count(False) for t in target_var]\n",
    "    \n",
    "    return input_var, target_var, input_len, target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_index):\n",
    "    idxs = list(map(lambda w: to_index[w] if w in to_index.keys() else to_index[\"<UNK>\"], seq))\n",
    "    return Variable(LongTensor(idxs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data load and Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DIR_PATH='../dataset/corpus/bAbI/en-10k/'\n",
    "flist = os.listdir(DIR_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=[]\n",
    "for f in flist:\n",
    "    #if f.endswith('train.txt'):\n",
    "        fname = DIR_PATH+f\n",
    "        #print(fname)\n",
    "        temp = open(fname,'r',encoding='utf-8').readlines()\n",
    "        temp = [t[:-1] for t in temp]\n",
    "        data.extend(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = open('../dataset/corpus/bAbI/en-10k/qa1_single-supporting-fact_train.txt').readlines()\n",
    "data = [d[:-1] for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for d in data:\n",
    "    if '?' in d:\n",
    "        count+=1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train=[]\n",
    "support=[]\n",
    "qa=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in data:\n",
    "\n",
    "    index = d.split(' ')[0]\n",
    "    if index=='1':\n",
    "        support=[]\n",
    "        qa=[]\n",
    "        \n",
    "    if '?' in d:\n",
    "        temp = d.split('\\t')\n",
    "        q = temp[0].strip().replace('?','').split(' ')[1:]+['?']\n",
    "        a = temp[1].split()+['</s>']\n",
    "        #f = [int(t)-1 for t in temp[2].split()]\n",
    "        stemp = deepcopy(support)\n",
    "\n",
    "        train.append([stemp,q,a])\n",
    "    else:\n",
    "        tokens = d.replace('.','').split(' ')[1:]+['</s>']\n",
    "        support.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "support ,q,a = list(zip(*train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Mary', 'moved', 'to', 'the', 'bathroom', '</s>'],\n",
       " ['John', 'went', 'to', 'the', 'hallway', '</s>']]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "support[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({2: 2000, 4: 2000, 6: 2000, 8: 2000, 10: 2000})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([len(s) for s in support])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(set(flatten(flatten(support))+flatten(q)+flatten(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index={'<PAD>':0,'</s>':1}\n",
    "for vo in vocab:\n",
    "    word2index[vo]=len(word2index)\n",
    "index2word = {v:k for k,v in word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in train:\n",
    "    for i,sup in enumerate(t[0]):\n",
    "        t[0][i] = prepare_sequence(sup,word2index).view(1,-1)\n",
    "    \n",
    "    t[1] = prepare_sequence(t[1],word2index).view(1,-1)\n",
    "    t[2] = prepare_sequence(t[2],word2index).view(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "support = torch.cat(train[12][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "  5  16  18   8   7  21\n",
       " 12  13  18   8   9  21\n",
       "  2  13  18   8  17  21\n",
       " 12  13  18   8  14  21\n",
       " 12  13  18   8   9  21\n",
       "  2  13  18   8  14  21\n",
       "[torch.LongTensor of size 6x6]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DMN(nn.Module):\n",
    "    def __init__(self, input_size,hidden_size,output_size):\n",
    "        super(DMN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed = nn.Embedding(input_size, hidden_size, padding_idx=0, sparse=True)\n",
    "        self.input_gru = nn.GRU(hidden_size, hidden_size,batch_first=True)\n",
    "        self.question_gru = nn.GRU(hidden_size, hidden_size,batch_first=True)\n",
    "        \n",
    "        self.gate = nn.Sequential(\n",
    "                            nn.Linear(hidden_size*4,hidden_size),\n",
    "                            nn.Tanh(),\n",
    "                            nn.Linear(hidden_size,1),\n",
    "                            nn.Sigmoid()\n",
    "                        )\n",
    "        \n",
    "        self.attention_gru =  nn.GRUCell(hidden_size, hidden_size)\n",
    "        self.memory_gru = nn.GRU(hidden_size,hidden_size,batch_first=True)\n",
    "        self.answer_gru = nn.GRU(hidden_size, hidden_size,batch_first=True)\n",
    "        self.answer_fc = nn.Linear(hidden_size,output_size)\n",
    "        \n",
    "    def init_hidden(self,inputs):\n",
    "        hidden = Variable(torch.zeros(1,inputs.size(0),self.hidden_size))\n",
    "        return hidden.cuda() if USE_CUDA else hidden\n",
    "\n",
    "    def memory_update(self,Cs,Gs):\n",
    "        pass\n",
    "\n",
    "    def forward(self,inputs,questions):\n",
    "        \n",
    "        C=[]\n",
    "        for input_ in inputs:\n",
    "            embeds = self.embed(input_)\n",
    "            hidden = self.init_hidden(inputs)\n",
    "            outputs,hidden = self.input_gru(embeds,hidden)\n",
    "            C.append(hidden.squeeze(0)) # B x T x D\n",
    "        \n",
    "        embeds = self.embed(questions)\n",
    "        hidden = self.init_hidden(questions)\n",
    "        outputs, hidden = self.question_gru(embeds,hidden)\n",
    "        \n",
    "        \n",
    "        \n",
    "        return hidden.squeeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = DMN(len(word2index),50,len(word2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "\n",
       "Columns 0 to 9 \n",
       " 0.4173  0.4438  0.7172 -0.5022 -0.1528 -0.2227 -0.4276 -0.2200 -0.3188 -0.3380\n",
       " 0.4938  0.2621  0.7536 -0.5704 -0.4606 -0.3523 -0.2763  0.0045 -0.4024 -0.2828\n",
       " 0.5056  0.4136  0.7094 -0.4888 -0.1466 -0.4241 -0.1926  0.1614 -0.3194 -0.2285\n",
       " 0.5455  0.2938  0.8005 -0.4930  0.0572 -0.3729 -0.3775 -0.0722 -0.0260 -0.3010\n",
       " 0.4938  0.2621  0.7536 -0.5704 -0.4606 -0.3523 -0.2763  0.0045 -0.4024 -0.2828\n",
       " 0.5440  0.2921  0.7982 -0.5086  0.0824 -0.3721 -0.3756 -0.0814  0.0139 -0.2941\n",
       "\n",
       "Columns 10 to 19 \n",
       " 0.1222 -0.2928  0.2636 -0.2284  0.1185  0.1745  0.4569  0.0064 -0.4305 -0.0360\n",
       " 0.0670 -0.1415  0.0685 -0.3162  0.1461 -0.1845  0.3521  0.0121 -0.3087 -0.0470\n",
       " 0.2300 -0.2021  0.0709 -0.3735 -0.0517  0.1506  0.3462 -0.0615 -0.3862 -0.1503\n",
       " 0.1159  0.0838  0.2641 -0.3576  0.0637  0.4873  0.4478 -0.2282 -0.3949 -0.0969\n",
       " 0.0670 -0.1415  0.0685 -0.3162  0.1461 -0.1845  0.3521  0.0121 -0.3087 -0.0470\n",
       " 0.1305  0.0611  0.2616 -0.3400  0.0831  0.5096  0.4586 -0.2562 -0.3966 -0.1449\n",
       "\n",
       "Columns 20 to 29 \n",
       " 0.3856 -0.2812 -0.1037 -0.0302 -0.5657 -0.2724 -0.1961  0.2719 -0.0635 -0.4307\n",
       " 0.4396  0.0458  0.0196  0.0119 -0.5696 -0.1845 -0.2073  0.3806 -0.0509 -0.1650\n",
       " 0.3092  0.3240 -0.3570  0.1894 -0.5883 -0.3454 -0.3914  0.0545 -0.0982 -0.1579\n",
       " 0.2381  0.2961 -0.3189 -0.2013 -0.5161 -0.2380 -0.2137  0.1251  0.0255 -0.0549\n",
       " 0.4396  0.0458  0.0196  0.0119 -0.5696 -0.1845 -0.2073  0.3806 -0.0509 -0.1650\n",
       " 0.2180  0.2938 -0.3284 -0.1824 -0.5186 -0.2467 -0.2338  0.1291  0.0136 -0.0541\n",
       "\n",
       "Columns 30 to 39 \n",
       "-0.1420  0.0553  0.2367 -0.4492 -0.3822 -0.0206  0.0213  0.1959 -0.0603 -0.1150\n",
       "-0.1236  0.2083  0.1088 -0.3467 -0.2882 -0.0394  0.2100  0.1118 -0.0743 -0.0133\n",
       "-0.0095  0.2487  0.0755 -0.5148 -0.1161 -0.0857  0.2766 -0.0175  0.1430  0.2774\n",
       "-0.1623  0.1547 -0.1052 -0.4767 -0.2300 -0.0645  0.1260  0.0752  0.2178  0.2062\n",
       "-0.1236  0.2083  0.1088 -0.3467 -0.2882 -0.0394  0.2100  0.1118 -0.0743 -0.0133\n",
       "-0.1506  0.1734 -0.0907 -0.4847 -0.2391 -0.0670  0.1180  0.0817  0.2322  0.1949\n",
       "\n",
       "Columns 40 to 49 \n",
       " 0.0157  0.0480 -0.0738 -0.3617 -0.3900  0.1739 -0.6929 -0.3458 -0.6314 -0.7306\n",
       " 0.0582 -0.0905 -0.0915 -0.2214 -0.3360  0.0486 -0.7814 -0.3292 -0.6692 -0.7035\n",
       " 0.0578  0.1535 -0.0011 -0.2022 -0.3493  0.5186 -0.7317 -0.2366 -0.7153 -0.4494\n",
       "-0.0449 -0.0052  0.0468 -0.0741 -0.3386  0.3738 -0.7738 -0.1501 -0.6347 -0.3384\n",
       " 0.0582 -0.0905 -0.0915 -0.2214 -0.3360  0.0486 -0.7814 -0.3292 -0.6692 -0.7035\n",
       "-0.0625 -0.0326  0.0427 -0.0851 -0.3439  0.3782 -0.7774 -0.1391 -0.6388 -0.3253\n",
       "[torch.FloatTensor of size 6x50]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(support,question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
