{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import nltk\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://github.com/rguthrie3/DeepDependencyParsingProblemSet/tree/master/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getBatch(batch_size,train_data):\n",
    "    random.shuffle(train_data)\n",
    "    sindex=0\n",
    "    eindex=batch_size\n",
    "    while eindex < len(train_data):\n",
    "        batch = train_data[sindex:eindex]\n",
    "        temp = eindex\n",
    "        eindex = eindex+batch_size\n",
    "        sindex = temp\n",
    "        yield batch\n",
    "    \n",
    "    if eindex >= len(train_data):\n",
    "        batch = train_data[sindex:]\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_vector(sents, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if w in word2index.keys() else word2index[\"<unk>\"], sents))\n",
    "    tensor = Variable(torch.LongTensor(idxs)).cuda() if USE_CUDA else  Variable(torch.LongTensor(idxs))\n",
    "    return tensor.view(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainsitionState(object):\n",
    "    \n",
    "    def __init__(self,tagged_sent):\n",
    "        self.root = ('ROOT','<root>',-1)\n",
    "        self.stack=[self.root]\n",
    "        self.buffer=[(s[0],s[1],i) for i,s in enumerate(tagged_sent)]\n",
    "        self.address = [s[0] for s in tagged_sent] + [self.root[0]]\n",
    "        self.arcs=[]\n",
    "        self.terminal=False\n",
    "        \n",
    "    def __str__(self):\n",
    "        return 'stack : %s \\nbuffer : %s' % (str([s[0] for s in self.stack]), str([b[0] for b in self.buffer]))\n",
    "    \n",
    "    def shift(self):\n",
    "        \n",
    "        if len(self.buffer)>=1:\n",
    "            self.stack.append(self.buffer.pop(0))\n",
    "        else:\n",
    "            print(\"Empty buffer\")\n",
    "            \n",
    "    def left_arc(self,relation=None):\n",
    "        \n",
    "        if len(self.stack)>=2:\n",
    "            arc={}\n",
    "            s2 = self.stack[-2]\n",
    "            s1 = self.stack[-1]\n",
    "            arc['graph_id'] = len(self.arcs)\n",
    "            arc['form'] = s1[0]\n",
    "            arc['addr'] = s1[2]\n",
    "            arc['head']=s2[2]\n",
    "            arc['pos'] = s1[1]\n",
    "            if relation:\n",
    "                arc['relation']=relation\n",
    "            self.arcs.append(arc)\n",
    "            self.stack.pop(-2)\n",
    "            \n",
    "        elif self.stack==[self.root]:\n",
    "            print(\"Element Lacking\")\n",
    "    \n",
    "    def right_arc(self,relation=None):\n",
    "        \n",
    "        if len(self.stack)>=2:\n",
    "            arc={}\n",
    "            s2 = self.stack[-2]\n",
    "            s1 = self.stack[-1]\n",
    "            arc['graph_id'] = len(self.arcs)\n",
    "            arc['form'] = s2[0]\n",
    "            arc['addr'] = s2[2]\n",
    "            arc['head']=s1[2]\n",
    "            arc['pos'] = s2[1]\n",
    "            if relation:\n",
    "                arc['relation']=relation\n",
    "            self.arcs.append(arc)\n",
    "            self.stack.pop(-1)\n",
    "            \n",
    "        elif self.stack==[self.root]:\n",
    "            print(\"Element Lacking\")\n",
    "    \n",
    "    def get_left_most(self,index):\n",
    "        left=['<NULL>','<NULL>',None]\n",
    "        \n",
    "        if index==None: return left\n",
    "        for arc in self.arcs:\n",
    "            if arc['head']==index:\n",
    "                left=[arc['form'],arc['pos'],arc['addr']]\n",
    "                break\n",
    "        return left\n",
    "    \n",
    "    def get_right_most(self,index):\n",
    "        right=['<NULL>','<NULL>',None]\n",
    "        \n",
    "        if index==None: return right\n",
    "        for arc in reversed(self.arcs):\n",
    "            if arc['head']==index:\n",
    "                right=[arc['form'],arc['pos'],arc['addr']]\n",
    "                break\n",
    "        return right\n",
    "    \n",
    "    def is_done(self):\n",
    "        return len(self.buffer)==0 and self.stack==[self.root]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack : ['ROOT'] \n",
      "buffer : ['He', 'has', 'good', 'control', '.']\n",
      "stack : ['ROOT', 'He', 'has'] \n",
      "buffer : ['good', 'control', '.']\n",
      "stack : ['ROOT', 'has'] \n",
      "buffer : ['good', 'control', '.']\n",
      "[{'head': 0, 'addr': 1, 'graph_id': 0, 'pos': 'VBZ', 'form': 'has'}]\n",
      "stack : ['ROOT', 'has', 'good', 'control'] \n",
      "buffer : ['.']\n",
      "stack : ['ROOT', 'has', 'control'] \n",
      "buffer : ['.']\n",
      "stack : ['ROOT', 'has'] \n",
      "buffer : ['.']\n",
      "stack : ['ROOT', 'has'] \n",
      "buffer : []\n",
      "stack : ['ROOT'] \n",
      "buffer : []\n",
      "[{'head': 0, 'addr': 1, 'graph_id': 0, 'pos': 'VBZ', 'form': 'has'}, {'head': 2, 'addr': 3, 'graph_id': 1, 'pos': 'NN', 'form': 'control'}, {'head': 3, 'addr': 1, 'graph_id': 2, 'pos': 'VBZ', 'form': 'has'}, {'head': 4, 'addr': 1, 'graph_id': 3, 'pos': 'VBZ', 'form': 'has'}, {'head': 1, 'addr': -1, 'graph_id': 4, 'pos': '<root>', 'form': 'ROOT'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = TrainsitionState(nltk.pos_tag(\"He has good control .\".split()))\n",
    "print(temp)\n",
    "temp.shift()\n",
    "temp.shift()\n",
    "print(temp)\n",
    "temp.left_arc()\n",
    "print(temp)\n",
    "print(temp.arcs)\n",
    "temp.shift()\n",
    "temp.shift()\n",
    "print(temp)\n",
    "temp.left_arc()\n",
    "print(temp)\n",
    "temp.right_arc()\n",
    "print(temp)\n",
    "temp.shift()\n",
    "temp.right_arc()\n",
    "print(temp)\n",
    "temp.right_arc()\n",
    "print(temp)\n",
    "print(temp.arcs)\n",
    "temp.is_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_tree(state,image_name):\n",
    "    graph = pydot.Dot(graph_type='graph')\n",
    "    for arc in state.arcs:\n",
    "        edge = pydot.Edge(arc['form'],state.address[arc['head']])\n",
    "        graph.add_edge(edge)\n",
    "    \n",
    "    graph.write_png('t_graph.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data load & Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feat(transition_state,word2index,tag2index,label2index=None):\n",
    "    word_feats=[]\n",
    "    tag_feats = []\n",
    "    \n",
    "    word_feats.append(transition_state.stack[-1][0]) if len(transition_state.stack)>=1 and \\\n",
    "    transition_state.stack[-1][0] in word2index.keys() else word_feats.append('<NULL>') # s1\n",
    "    word_feats.append(transition_state.stack[-2][0]) if len(transition_state.stack)>=2 and \\\n",
    "    transition_state.stack[-2][0] in word2index.keys() else word_feats.append('<NULL>') # s2\n",
    "    word_feats.append(transition_state.stack[-3][0]) if len(transition_state.stack)>=3 and \\\n",
    "    transition_state.stack[-3][0] in word2index.keys() else word_feats.append('<NULL>') # s3\n",
    "    \n",
    "    tag_feats.append(transition_state.stack[-1][1]) if len(transition_state.stack)>=1 and \\\n",
    "    transition_state.stack[-1][1] in tag2index.keys() else tag_feats.append('<NULL>') # st1\n",
    "    tag_feats.append(transition_state.stack[-2][1]) if len(transition_state.stack)>=2 and \\\n",
    "    transition_state.stack[-2][1] in tag2index.keys() else tag_feats.append('<NULL>') # st2\n",
    "    tag_feats.append(transition_state.stack[-3][1]) if len(transition_state.stack)>=3 and \\\n",
    "    transition_state.stack[-3][1] in tag2index.keys() else tag_feats.append('<NULL>') # st3\n",
    "    \n",
    "    \n",
    "    word_feats.append(transition_state.buffer[0][0]) if len(transition_state.buffer)>=1 and \\\n",
    "    transition_state.buffer[0][0] in word2index.keys() else word_feats.append('<NULL>') # b1\n",
    "    word_feats.append(transition_state.buffer[1][0]) if len(transition_state.buffer)>=2 and \\\n",
    "    transition_state.buffer[1][0] in word2index.keys() else word_feats.append('<NULL>') # b2\n",
    "    word_feats.append(transition_state.buffer[2][0]) if len(transition_state.buffer)>=3 and \\\n",
    "    transition_state.buffer[2][0] in word2index.keys() else word_feats.append('<NULL>') # b3\n",
    "    \n",
    "    tag_feats.append(transition_state.buffer[0][1]) if len(transition_state.buffer)>=1 and \\\n",
    "    transition_state.buffer[0][1] in tag2index.keys() else tag_feats.append('<NULL>') # bt1\n",
    "    tag_feats.append(transition_state.buffer[1][1]) if len(transition_state.buffer)>=2 and \\\n",
    "    transition_state.buffer[1][1] in tag2index.keys() else tag_feats.append('<NULL>') # bt2\n",
    "    tag_feats.append(transition_state.buffer[2][1]) if len(transition_state.buffer)>=3 and \\\n",
    "    transition_state.buffer[2][1] in tag2index.keys() else tag_feats.append('<NULL>') # bt3\n",
    "    \n",
    "    \n",
    "    lc_s1 = transition_state.get_left_most(transition_state.stack[-1][2]) if len(transition_state.stack)>=1 \\\n",
    "    else transition_state.get_left_most(None)\n",
    "    rc_s1 = transition_state.get_right_most(transition_state.stack[-1][2]) if len(transition_state.stack)>=1 \\\n",
    "    else transition_state.get_right_most(None)\n",
    "    \n",
    "    lc_s2 = transition_state.get_left_most(transition_state.stack[-2][2]) if len(transition_state.stack)>=2 \\\n",
    "    else transition_state.get_left_most(None)\n",
    "    rc_s2 = transition_state.get_right_most(transition_state.stack[-2][2]) if len(transition_state.stack)>=2 \\\n",
    "    else transition_state.get_right_most(None)\n",
    "    \n",
    "    words, tags, _ = zip(*[lc_s1,rc_s1,lc_s2,rc_s2])\n",
    "    \n",
    "    word_feats.extend(words)\n",
    "    \n",
    "    tag_feats.extend(tags)\n",
    "    \n",
    "    \n",
    "    return make_vector(word_feats,word2index), make_vector(tag_feats,tag2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = open('../DeepDependencyParsingProblemSet/data/train.txt','r').readlines()\n",
    "vocab = open('../DeepDependencyParsingProblemSet/data/vocab.txt','r').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [[nltk.pos_tag(d.split('|||')[0].split()), d.split('|||')[1][:-1].split()] for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x,train_y = list(zip(*train_data))\n",
    "train_x_f = flatten(train_x)\n",
    "sents,pos_tags = list(zip(*train_x_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag2index = {v:i for i,v in enumerate(set(pos_tags))}\n",
    "tag2index['<root>']=len(tag2index)\n",
    "tag2index['<NULL>']=len(tag2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = [v.split('\\t')[0] for v in vocab]\n",
    "word2index = {v:i for i,v in enumerate(vocab)}\n",
    "word2index['ROOT'] = len(word2index)\n",
    "word2index['<NULL>'] = len(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "actions = ['SHIFT','REDUCE_L','REDUCE_R']\n",
    "action2index = {v:i for i,v in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_train=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tx,ty in train_data:\n",
    "    state = TrainsitionState(tx)\n",
    "    transition = ty+['REDUCE_R']\n",
    "    while len(transition)!=0:\n",
    "        feat = get_feat(state,word2index,tag2index)\n",
    "        action = transition.pop(0)\n",
    "        actionTensor = Variable(torch.LongTensor([action2index[action]])).view(1,-1).cuda() if USE_CUDA \\\n",
    "        else Variable(torch.LongTensor([action2index[action]])).view(1,-1)\n",
    "        p_train.append([feat,actionTensor])\n",
    "        if action=='SHIFT':\n",
    "            state.shift()\n",
    "        elif action=='REDUCE_R':\n",
    "            state.right_arc()\n",
    "        elif action=='REDUCE_L':\n",
    "            state.left_arc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Variable containing:\n",
       "   9151  9152  9152  2106     2   353  9152  9152  9152  9152\n",
       "  [torch.LongTensor of size 1x10], Variable containing:\n",
       "     43    44    44    30    34    29    44    44    44    44\n",
       "  [torch.LongTensor of size 1x10]), Variable containing:\n",
       "  0\n",
       " [torch.LongTensor of size 1x1]]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralDependencyParser(nn.Module):\n",
    "    \n",
    "    def __init__(self,w_size,w_embed_dim,t_size,t_embed_dim,hidden_size,target_size):\n",
    "        \n",
    "        super(NeuralDependencyParser, self).__init__()\n",
    "        \n",
    "        self.w_embed =  nn.Embedding(w_size,w_embed_dim)\n",
    "        self.t_embed = nn.Embedding(t_size,t_embed_dim)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.target_size = target_size\n",
    "        self.linear = nn.Linear((w_embed_dim+t_embed_dim)*10,self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size,self.target_size)\n",
    "        \n",
    "        self.w_embed.weight.data.uniform_(-0.01, 0.01) # init\n",
    "        self.t_embed.weight.data.uniform_(-0.01, 0.01) # init\n",
    "        \n",
    "    def forward(self,words,tags):\n",
    "        \n",
    "        wem = self.w_embed(words).view(words.size(0),-1)\n",
    "        tem = self.t_embed(tags).view(tags.size(0),-1)\n",
    "        inputs = torch.cat([wem,tem],1)\n",
    "        h1 = torch.pow(self.linear(inputs),3) # cube function\n",
    "        preds = -self.out(h1)\n",
    "        return F.log_softmax(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "STEP = 5\n",
    "W_EMBED_SIZE = 50\n",
    "T_EMBED_SIZE = 10\n",
    "HIDDEN_SIZE=512\n",
    "LR = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = NeuralDependencyParser(len(word2index),W_EMBED_SIZE,len(tag2index),T_EMBED_SIZE,HIDDEN_SIZE,len(action2index))\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "losses=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.09684097767\n",
      "0.737392584085\n",
      "0.618838954717\n",
      "0.693304691613\n",
      "1.06236269116\n",
      "1.83809881449\n",
      "4.97554338366\n",
      "9.61246664047\n",
      "12.1350490642\n",
      "12.1166139519\n",
      "9.60266982526\n",
      "8.15794750815\n",
      "7.09933471501\n",
      "6.81534143507\n",
      "6.49781926632\n",
      "5.87462216884\n",
      "5.22908880115\n",
      "4.69333528399\n",
      "4.52874009252\n",
      "4.62807468295\n",
      "4.31034019113\n",
      "3.43664512917\n",
      "3.45341291964\n",
      "3.25474904835\n",
      "3.85090582579\n",
      "3.66804424644\n",
      "3.28917575121\n",
      "3.16772120774\n",
      "3.17027136356\n",
      "2.78850893259\n",
      "2.66806945607\n",
      "3.0550786984\n",
      "3.04333923131\n",
      "3.15591797143\n",
      "2.94243722975\n",
      "2.90874896646\n",
      "2.83968224376\n",
      "2.8125838387\n",
      "3.11807466447\n",
      "3.24308602437\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-192-54dbb8c3c0ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mUSE_CUDA\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(getBatch(32,p_train)):\n",
    "    \n",
    "    model.zero_grad()\n",
    "    inputs, targets = list(zip(*batch))\n",
    "    words, tags = list(zip(*inputs))\n",
    "    words = torch.cat(words)\n",
    "    tags = torch.cat(tags)\n",
    "    targets = torch.cat(targets)\n",
    "    preds = model(words,tags)\n",
    "    loss = loss_function(preds,targets.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(loss.data.cpu().tolist()[0] if USE_CUDA else loss.data.tolist()[0])\n",
    "    \n",
    "    if i % 100==0:\n",
    "        print(np.mean(losses))\n",
    "        losses=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0932177305221558"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
