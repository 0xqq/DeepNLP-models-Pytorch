{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.Tree Recursive Neural Networks and Constituency Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture14-TreeRNNs.pdf\n",
    "* https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter, OrderedDict\n",
    "import nltk\n",
    "from copy import deepcopy\n",
    "import os\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if USE_CUDA else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if USE_CUDA else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if USE_CUDA else torch.ByteTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getBatch(batch_size,train_data):\n",
    "    random.shuffle(train_data)\n",
    "    sindex=0\n",
    "    eindex=batch_size\n",
    "    while eindex < len(train_data):\n",
    "        batch = train_data[sindex:eindex]\n",
    "        temp = eindex\n",
    "        eindex = eindex+batch_size\n",
    "        sindex = temp\n",
    "        yield batch\n",
    "    \n",
    "    if eindex >= len(train_data):\n",
    "        batch = train_data[sindex:]\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data load and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stanford Sentiment Treebank(https://nlp.stanford.edu/sentiment/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2 (2 (1 (0 (1 (2 It) (1 (2 would) (1 (2 be) (1 (1 disingenuous) (3 (2 to) (4 (2 (2 call) (2 Reno)) (4 (2 a) (4 (4 great) (2 film))))))))) (2 ,)) (2 but)) (2 (2 you) (2 (2 can) (2 (2 say) (2 (2 (2 that) (2 (2 about) (2 most))) (2 (2 of) (2 (2 (2 the) (2 flicks)) (2 (2 (3 moving) (2 (2 (2 in) (2 and)) (1 out))) (2 (2 of) (2 (2 the) (2 multiplex))))))))))) (2 .))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample = random.choice(open('../dataset/trees/train.txt','r',encoding='utf-8').readlines())\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree Class "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "borrowed code from https://github.com/bogatyy/cs224d/tree/master/assignment3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Node:  # a node in the tree\n",
    "    def __init__(self, label, word=None):\n",
    "        self.label = label\n",
    "        self.word = word\n",
    "        self.parent = None  # reference to parent\n",
    "        self.left = None  # reference to left child\n",
    "        self.right = None  # reference to right child\n",
    "        # true if I am a leaf (could have probably derived this from if I have\n",
    "        # a word)\n",
    "        self.isLeaf = False\n",
    "        # true if we have finished performing fowardprop on this node (note,\n",
    "        # there are many ways to implement the recursion.. some might not\n",
    "        # require this flag)\n",
    "\n",
    "    def __str__(self):\n",
    "        if self.isLeaf:\n",
    "            return '[{0}:{1}]'.format(self.word, self.label)\n",
    "        return '({0} <- [{1}:{2}] -> {3})'.format(self.left, self.word, self.label, self.right)\n",
    "\n",
    "\n",
    "class Tree:\n",
    "\n",
    "    def __init__(self, treeString, openChar='(', closeChar=')'):\n",
    "        tokens = []\n",
    "        self.open = '('\n",
    "        self.close = ')'\n",
    "        for toks in treeString.strip().split():\n",
    "            tokens += list(toks)\n",
    "        self.root = self.parse(tokens)\n",
    "        # get list of labels as obtained through a post-order traversal\n",
    "        self.labels = get_labels(self.root)\n",
    "        self.num_words = len(self.labels)\n",
    "\n",
    "    def parse(self, tokens, parent=None):\n",
    "        assert tokens[0] == self.open, \"Malformed tree\"\n",
    "        assert tokens[-1] == self.close, \"Malformed tree\"\n",
    "\n",
    "        split = 2  # position after open and label\n",
    "        countOpen = countClose = 0\n",
    "\n",
    "        if tokens[split] == self.open:\n",
    "            countOpen += 1\n",
    "            split += 1\n",
    "        # Find where left child and right child split\n",
    "        while countOpen != countClose:\n",
    "            if tokens[split] == self.open:\n",
    "                countOpen += 1\n",
    "            if tokens[split] == self.close:\n",
    "                countClose += 1\n",
    "            split += 1\n",
    "\n",
    "        # New node\n",
    "        node = Node(int(tokens[1]))  # zero index labels\n",
    "\n",
    "        node.parent = parent\n",
    "\n",
    "        # leaf Node\n",
    "        if countOpen == 0:\n",
    "            node.word = ''.join(tokens[2:-1]).lower()  # lower case?\n",
    "            node.isLeaf = True\n",
    "            return node\n",
    "\n",
    "        node.left = self.parse(tokens[2:split], parent=node)\n",
    "        node.right = self.parse(tokens[split:-1], parent=node)\n",
    "\n",
    "        return node\n",
    "\n",
    "    def get_words(self):\n",
    "        leaves = getLeaves(self.root)\n",
    "        words = [node.word for node in leaves]\n",
    "        return words\n",
    "\n",
    "def loadTrees(dataSet='train'):\n",
    "    \"\"\"\n",
    "    Loads training trees. Maps leaf node words to word ids.\n",
    "    \"\"\"\n",
    "    file = '../dataset/trees/%s.txt' % dataSet\n",
    "    print(\"Loading %s trees..\" % dataSet)\n",
    "    with open(file, 'r',encoding='utf-8') as fid:\n",
    "        trees = [Tree(l) for l in fid.readlines()]\n",
    "\n",
    "    return trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train trees..\n"
     ]
    }
   ],
   "source": [
    "train_data = loadTrees('train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Vocab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = list(set(flatten([t.get_words() for t in train_data])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2index={}\n",
    "for vo in vocab:\n",
    "    if vo not in word2index.keys():\n",
    "        word2index[vo]=len(word2index)\n",
    "        \n",
    "index2word = {v:k for k,v in word2index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RNTN(nn.Module):\n",
    "    \n",
    "    def __init__(self,word2index,hidden_size,output_size):\n",
    "        super(RNTN,self).__init__()\n",
    "        \n",
    "        self.word2index = word2index\n",
    "        self.embed = nn.Embedding(len(word2index),hidden_size)\n",
    "        self.V = nn.ParameterList([nn.Parameter(torch.randn(hidden_size*2,hidden_size*2)) for _ in range(hidden_size)]) # Tensor\n",
    "        self.W = nn.Parameter(torch.randn(hidden_size*2,hidden_size))\n",
    "        self.W_out = nn.Linear(hidden_size,output_size)\n",
    "        \n",
    "    def tree_propagation(self,node):\n",
    "        \n",
    "        recursive_tensor = OrderedDict()\n",
    "        current=None\n",
    "        if node.isLeaf:\n",
    "            tensor = Variable(LongTensor([self.word2index[node.word]]))\n",
    "            current = self.embed(tensor) # 1xD\n",
    "        else:\n",
    "            recursive_tensor.update(self.tree_propagation(node.left))\n",
    "            recursive_tensor.update(self.tree_propagation(node.right))\n",
    "            \n",
    "            concated = torch.cat([recursive_tensor[node.left],recursive_tensor[node.right]],1) # 1x2D\n",
    "            xVx=[] \n",
    "            for i,v in enumerate(self.V):\n",
    "                xVx.append(torch.matmul(torch.matmul(concated,v),concated.transpose(0,1)))\n",
    "            \n",
    "            xVx = torch.cat(xVx,1) # 1xD\n",
    "            Wx = torch.matmul(concated,self.W) # 1xD\n",
    "\n",
    "            current = F.tanh(xVx+Wx)\n",
    "        recursive_tensor[node]=current\n",
    "        return recursive_tensor\n",
    "        \n",
    "    def forward(self,Trees,root_only=False):\n",
    "        \n",
    "        propagated=[]\n",
    "        if not isinstance(Trees,list):\n",
    "            Trees = [Trees]\n",
    "            \n",
    "        for Tree in Trees:\n",
    "            recursive_tensor = self.tree_propagation(Tree.root)\n",
    "            if root_only:\n",
    "                recursive_tensor = recursive_tensor[Tree.root]\n",
    "                propagated.append(recursive_tensor)\n",
    "            else:\n",
    "                recursive_tensor = [tensor for node,tensor in recursive_tensor.items()]\n",
    "                propagated.extend(recursive_tensor)\n",
    "        \n",
    "        propagated = torch.cat(propagated)\n",
    "              \n",
    "        return F.log_softmax(self.W_out(propagated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 35\n",
    "ROOT_ONLY = False\n",
    "BATCH_SIZE = 20\n",
    "EPOCH = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = RNTN(word2index,HIDDEN_SIZE,5)\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/5] mean_loss : 1.76\n",
      "[0/5] mean_loss : 1.62\n",
      "[0/5] mean_loss : 1.45\n",
      "[0/5] mean_loss : 1.36\n",
      "[0/5] mean_loss : 1.30\n",
      "[1/5] mean_loss : 1.25\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    losses=[]\n",
    "    for i, batch in enumerate(getBatch(BATCH_SIZE,train)):\n",
    "        \n",
    "        if ROOT_ONLY:\n",
    "            labels = [tree.labels[-1] for tree in batch]\n",
    "            labels = Variable(LongTensor(labels))\n",
    "        else:\n",
    "            labels = [tree.labels for tree in batch]\n",
    "            labels = Variable(LongTensor(flatten(labels)))\n",
    "        \n",
    "        model.zero_grad()\n",
    "        preds = model(batch,ROOT_ONLY)\n",
    "        \n",
    "        loss = loss_function(preds,labels)\n",
    "        losses.append(loss.data.tolist()[0])\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 100==0:\n",
    "            print('[%d/%d] mean_loss : %.2f' % (epoch,EPOCH,np.mean(losses)))\n",
    "            losses=[]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://github.com/nearai/pytorch-tools # Dynamic batch using TensorFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
